{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **5-output-parsers**\n",
        "\n",
        "## **ü§ñ Introduction**\n",
        "- **Output Parsers**: Tools or methods that help **format** the LLM‚Äôs answer into a **desired structure** (e.g., JSON, CSV, bullet points).\n",
        "\n",
        "## **‚öôÔ∏è Setup**\n",
        "1. **Clone** or **Download** the GitHub repository.\n",
        "2. In **terminal**:\n",
        "   ```\n",
        "   cd project_name\n",
        "   pyenv local 3.11.4\n",
        "   poetry install\n",
        "   poetry shell\n",
        "   ```\n",
        "3. Launch **Jupyter Lab**:\n",
        "   ```\n",
        "   jupyter lab\n",
        "   ```\n",
        "   - Open the **`005-output-parsers.ipynb`** file in the notebooks folder.\n",
        "4. **View Code** in your preferred editor (e.g., VS Code):\n",
        "   - Open **`005-output-parsers.py`** to see or edit parser logic.\n",
        "\n",
        "---\n",
        "\n",
        "## **üîê Create Your `.env` File**\n",
        "- Rename **`.env.example`** to **`.env`**.\n",
        "- Insert your **API keys**:\n",
        "  ```\n",
        "  OPENAI_API_KEY=your_openai_api_key\n",
        "  LANGCHAIN_TRACING_V2=true\n",
        "  LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
        "  LANGCHAIN_API_KEY=your_langchain_api_key\n",
        "  LANGCHAIN_PROJECT=your_project_name\n",
        "  ```\n",
        "- This LangSmith project is named **`005-output-parsers`**.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìä Track Operations**\n",
        "- You can **monitor usage and costs** in **LangSmith**:\n",
        "  ```\n",
        "  smith.langchain.com\n",
        "  ```\n",
        "\n",
        "> **üí° Pro Tip**: With **output parsers**, you can **standardize** how the LLM‚Äôs text is returned, making it easier to integrate with downstream processes (like data analysis or UI rendering).\n"
      ],
      "metadata": {
        "id": "deNmG-ZV2M8e"
      },
      "id": "deNmG-ZV2M8e"
    },
    {
      "cell_type": "markdown",
      "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291",
      "metadata": {
        "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291"
      },
      "source": [
        "## Connect with the .env file located in the same directory of this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a",
      "metadata": {
        "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a10870-432e-4818-aa5e-6be24c579d39",
      "metadata": {
        "id": "e9a10870-432e-4818-aa5e-6be24c579d39"
      },
      "outputs": [],
      "source": [
        "#pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
      "metadata": {
        "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
      "metadata": {
        "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80"
      },
      "source": [
        "#### Install LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa888f7-3718-4829-8645-30acb43db51f",
      "metadata": {
        "id": "6fa888f7-3718-4829-8645-30acb43db51f"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
      "metadata": {
        "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
      "metadata": {
        "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941"
      },
      "source": [
        "## Connect with an LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d5d5b71-b26a-4cd5-9765-019077a67141",
      "metadata": {
        "id": "4d5d5b71-b26a-4cd5-9765-019077a67141"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
      "metadata": {
        "id": "148df8e0-361d-4ddd-8709-af48fa1648d1"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
      "metadata": {
        "id": "a1998155-91de-4cbc-bc88-8d77beefb51b"
      },
      "source": [
        "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27",
      "metadata": {
        "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27"
      },
      "source": [
        "## LLM Model\n",
        "* The trend before the launch of chatGPT-4.\n",
        "* See LangChain documentation about LLM Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/llms/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92628f2-62e8-436c-92d4-e849de7744ad",
      "metadata": {
        "id": "e92628f2-62e8-436c-92d4-e849de7744ad"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llmModel = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31346a0c-ee9c-4a80-8e8a-85ef682df7c3",
      "metadata": {
        "id": "31346a0c-ee9c-4a80-8e8a-85ef682df7c3"
      },
      "source": [
        "## Chat Model\n",
        "* The general trend after the launch of chatGPT-4.\n",
        "    * Frequently known as \"Chatbot\".\n",
        "    * Conversation between Human and AI.\n",
        "    * Can have a system prompt defining the tone or the role of the AI.\n",
        "* See LangChain documentation about Chat Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/chat/).\n",
        "* By default we will work with ChatOpenAI. See [here](https://python.langchain.com/v0.1/docs/integrations/chat/openai/) the LangChain documentation page about it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7",
      "metadata": {
        "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4721c4a4-56d6-48e2-b9db-0de0309c492a",
      "metadata": {
        "id": "4721c4a4-56d6-48e2-b9db-0de0309c492a"
      },
      "source": [
        "## Parsing Outputs\n",
        "* See the corresponding LangChain Documentation page [here](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/).\n",
        "* Language models output text. But sometimes we would like to have those answers in a different format, like a JSON dictionary or a XML document. In order to achieve that, we use the Output Parsers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177b19be-ea45-4ecb-a4dc-914da7958de8",
      "metadata": {
        "id": "177b19be-ea45-4ecb-a4dc-914da7958de8"
      },
      "outputs": [],
      "source": [
        "# ü§ñ Import PromptTemplate to define a structured prompt\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# üóÑ Import SimpleJsonOutputParser to parse LLM output into JSON\n",
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "\n",
        "# üè∑ Create a prompt that instructs the model to return a JSON object with an `answer` key\n",
        "json_prompt = PromptTemplate.from_template(\n",
        "    \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
        ")\n",
        "\n",
        "# üõ† Instantiate the parser that will handle the JSON output\n",
        "json_parser = SimpleJsonOutputParser()\n",
        "\n",
        "# üîó Chain the prompt, LLM, and parser together\n",
        "json_chain = json_prompt | llmModel | json_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Prompt Definition**:  \n",
        "   - `json_prompt` directs the LLM to produce an **answer** inside a **JSON object**.  \n",
        "   - By specifying `\"Return a JSON object with an 'answer' key...\"`, you shape how the model formats its output.\n",
        "\n",
        "2. **Parser Setup**:  \n",
        "   - `SimpleJsonOutputParser` automatically converts the LLM‚Äôs textual response into **valid JSON**.  \n",
        "   - If the LLM follows the prompt, the parser will extract the `\"answer\"` field without manual parsing.\n",
        "\n",
        "3. **Chaining**:  \n",
        "   - `json_chain = json_prompt | llmModel | json_parser` creates a **pipeline**:  \n",
        "     1. **Prompt** is formatted with user input.  \n",
        "     2. **llmModel** generates text following the prompt‚Äôs instructions.  \n",
        "     3. **json_parser** ensures the final output is a **Python dictionary** (or raises an error if the format is incorrect).\n",
        "\n",
        "### **üí° Why Use an Output Parser?**\n",
        "- **Consistent Structure**: You can rely on a **known** schema (e.g., `{\"answer\": \"...\"} `).  \n",
        "- **Reduced Post-Processing**: No need to manually parse or guess the LLM‚Äôs formatting.  \n",
        "- **Automation-Friendly**: The parsed JSON is ready for further steps in your workflow, like storing results or passing them to another function.\n",
        "\n",
        "> **üöÄ Pro Tip**: Combine output parsers with **chains** to build complex pipelines‚Äîensuring each step receives **structured** data rather than free-form text."
      ],
      "metadata": {
        "id": "6licVr0S2cZW"
      },
      "id": "6licVr0S2cZW"
    },
    {
      "cell_type": "markdown",
      "id": "33eb4ed2-f174-4177-b38d-b03397e4f2a7",
      "metadata": {
        "id": "33eb4ed2-f174-4177-b38d-b03397e4f2a7"
      },
      "source": [
        "#### The previous prompt template includes the parser instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "047f7986-5502-4205-9a7a-e7e7e26f51a3",
      "metadata": {
        "id": "047f7986-5502-4205-9a7a-e7e7e26f51a3",
        "outputId": "bf66f0d9-1249-482f-cfda-dcc9b422edf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Return a JSON object.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json_parser.get_format_instructions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecb755d4-4a0f-486a-99dd-ea1873eb7c78",
      "metadata": {
        "id": "ecb755d4-4a0f-486a-99dd-ea1873eb7c78",
        "outputId": "317ae0db-74e8-4c2e-b5cf-4879b0d9239b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answer': 'Russia'}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json_chain.invoke({\"question\": \"What is the biggest country?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1181d87-13df-4abf-9910-0b44618642de",
      "metadata": {
        "id": "f1181d87-13df-4abf-9910-0b44618642de"
      },
      "source": [
        "#### Optionally, you can use Pydantic to define a custom output format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdcf2860-9b94-46f9-94f8-81ce173b8ca4",
      "metadata": {
        "id": "cdcf2860-9b94-46f9-94f8-81ce173b8ca4"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser  # üóÉ Import the JSON Output Parser\n",
        "from langchain_core.prompts import PromptTemplate           # üè∑ For creating structured prompts\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field     # üì¶ Pydantic BaseModel & Field for defining a schema\n",
        "from langchain_openai import ChatOpenAI                     # ü§ñ ChatOpenAI to interface with OpenAI's chat models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "873de824-c105-4e36-b4a7-d4bb498efc00",
      "metadata": {
        "id": "873de824-c105-4e36-b4a7-d4bb498efc00"
      },
      "outputs": [],
      "source": [
        "# Define a Pydantic Object with the desired output format.\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"question to set up a joke\")     #  \"setup\" is the initial part of the joke\n",
        "    punchline: str = Field(description=\"answer to resolve the joke\")#  \"punchline\" is the comedic conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7006219-e78f-4f1f-8d8a-559679f84845",
      "metadata": {
        "id": "c7006219-e78f-4f1f-8d8a-559679f84845",
        "outputId": "173970b8-c948-4db2-ee27-d9e6aa60ca26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'setup': \"Why couldn't the bicycle find its way home?\",\n",
              " 'punchline': 'Because it lost its bearings!'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the parser referring the Pydantic Object\n",
        "parser = JsonOutputParser(pydantic_object=Joke)  # üõ† Ensures LLM output follows the Joke schema\n",
        "\n",
        "# Add the parser format instructions in the prompt definition.\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# Create a chain with the prompt and the parser\n",
        "chain = prompt | chatModel | parser\n",
        "\n",
        "chain.invoke({\"query\": \"Tell me a joke.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üîé How This Works**\n",
        "1. **Pydantic Schema** (`Joke`):\n",
        "   - **`setup`**: The ‚Äúlead-in‚Äù question or statement for the joke.  \n",
        "   - **`punchline`**: The comedic resolution or answer.\n",
        "\n",
        "2. **`JsonOutputParser`**:\n",
        "   - Instructs the model to return output that **matches** the `Joke` schema.\n",
        "   - If the model deviates from this structure, the parser may throw an error or attempt to correct it.\n",
        "\n",
        "3. **Prompt Definition**:\n",
        "   - The template includes a `{format_instructions}` placeholder, which is populated by `parser.get_format_instructions()`.\n",
        "   - This ensures the model knows exactly how to **format** its response as valid JSON.\n",
        "\n",
        "4. **Chaining**:\n",
        "   - `prompt | chatModel | parser` creates a **pipeline**:\n",
        "     1. **Prompt** is prepared with the user‚Äôs query.\n",
        "     2. **chatModel** generates text following the prompt‚Äôs rules.\n",
        "     3. **parser** enforces the Pydantic schema, giving you a **Python object** (`Joke`) with `setup` and `punchline`.\n",
        "\n",
        "### **üí° Why Use a Custom Schema?**\n",
        "- **Structured Output**: Instead of free-form text, you get **key-value** pairs matching your domain (e.g., `setup`, `punchline`).\n",
        "- **Validation**: Pydantic ensures fields are present and in the correct format.\n",
        "- **Ease of Use**: Downstream code can rely on a well-defined object rather than unpredictable strings.\n",
        "\n",
        "> **‚öôÔ∏è Pro Tip**: You can extend this approach to **any** structured data (like events, product listings, or user profiles) by defining a suitable Pydantic model and letting `JsonOutputParser` handle the rest."
      ],
      "metadata": {
        "id": "7LHms5vQ3Rck"
      },
      "id": "7LHms5vQ3Rck"
    },
    {
      "cell_type": "markdown",
      "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444",
      "metadata": {
        "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444"
      },
      "source": [
        "## How to execute the code from Visual Studio Code\n",
        "* In Visual Studio Code, see the file 005-output-parsers.py\n",
        "* In terminal, make sure you are in the directory of the file and run:\n",
        "    * python 005-output-parsers.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069",
      "metadata": {
        "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}