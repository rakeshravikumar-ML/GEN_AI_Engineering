{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **🔗 Connect with an LLM**\n",
        "\n",
        "## **🤖 Start Talking with ChatGPT**\n",
        "- **Input**: The prompt we send to the LLM.  \n",
        "- **Output**: The response from the LLM.  \n",
        "- We can **switch LLMs** and use several different models.\n",
        "\n",
        "---\n",
        "\n",
        "## **💡 LangChain’s Two Main LLM Types**\n",
        "1. **LLM Model (Text-Completion)**  \n",
        "   - Produces a block of text based on a single prompt.  \n",
        "2. **Chat Model**  \n",
        "   - Converses with a sequence of messages, featuring roles like **system** and **human**.\n",
        "\n",
        "**🔍 Note**:  \n",
        "- While documentation sometimes blurs the line, **both** text-completion and chat models are LLMs.  \n",
        "- Chat models typically handle **system** (context), **human** (user input), and **assistant** (AI response) messages.\n",
        "\n",
        "---\n",
        "\n",
        "## **🎉 Rise of the Chat Model**\n",
        "- Since the launch of ChatGPT, **chat-based LLMs** have become the most popular format.  \n",
        "- They are used in a wide range of applications, from **Q&A** to **multi-turn dialogues**.\n",
        "\n",
        "---\n",
        "\n",
        "## **📜 List of LLMs Compatible with LangChain**\n",
        "- Refer to the **official documentation** for an updated list of supported LLMs.\n",
        "\n",
        "---\n",
        "\n",
        "## **⚙️ Setup**\n",
        "1. **Clone or Download** the GitHub repository to your local machine.  \n",
        "2. In **terminal**:\n",
        "   ```\n",
        "   cd project_name\n",
        "   pyenv local 3.11.4\n",
        "   poetry install\n",
        "   poetry shell\n",
        "   ```\n",
        "3. To open the notebook with Jupyter Notebooks:\n",
        "   ```\n",
        "   jupyter lab\n",
        "   ```\n",
        "   - Navigate to the **notebooks folder** and open the `001-connect-llm.ipynb` file.\n",
        "\n",
        "4. **View Code** in Visual Studio Code or another editor:\n",
        "   - Open the project folder.\n",
        "   - Locate and open `001-connect-llm.py`.\n",
        "\n",
        "---\n",
        "\n",
        "## **🔐 Create Your `.env` File**\n",
        "- The GitHub repo includes a file named **`.env.example`**.\n",
        "- Rename it to **`.env`** and add your confidential API keys:\n",
        "  ```\n",
        "  OPENAI_API_KEY=your_openai_api_key\n",
        "  LANGCHAIN_TRACING_V2=true\n",
        "  LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
        "  LANGCHAIN_API_KEY=your_langchain_api_key\n",
        "  LANGCHAIN_PROJECT=your_project_name\n",
        "  ```\n",
        "- This project is labeled **`001-connect-llm`** in **LangSmith**.\n",
        "\n",
        "---\n",
        "\n",
        "## **📊 Track Operations**\n",
        "- From now on, you can **monitor usage and costs** for this project in **LangSmith**:\n",
        "  ```\n",
        "  smith.langchain.com\n",
        "  ```\n",
        "\n"
      ],
      "metadata": {
        "id": "yffaYAaOCieV"
      },
      "id": "yffaYAaOCieV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **New Method**"
      ],
      "metadata": {
        "id": "GZraI6tECunH"
      },
      "id": "GZraI6tECunH"
    },
    {
      "cell_type": "markdown",
      "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291",
      "metadata": {
        "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291"
      },
      "source": [
        "## Connect with the .env file located in the same directory of this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a",
      "metadata": {
        "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a10870-432e-4818-aa5e-6be24c579d39",
      "metadata": {
        "id": "e9a10870-432e-4818-aa5e-6be24c579d39"
      },
      "outputs": [],
      "source": [
        "#pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e89510f",
      "metadata": {
        "id": "5e89510f"
      },
      "source": [
        "#### How to install with pip the exact same version listed in the pyproject.toml file\n",
        "\n",
        "To install a specific version of a package using `pip`, you should specify the version in the install command. If you see the pyproject.toml, you will see that the version listed for python-dotenv is `python-dotenv = \"^1.0.1\"`. This notation means to install the version `1.0.1` or any minor updates that still maintain compatibility (i.e., `1.x.x`).\n",
        "\n",
        "For `pip`, which doesn't natively support the caret (`^`) version specifier directly in the command line, you'll want to specify the exact version you need or use comparison operators to define a range. If you only need exactly version `1.0.1`, you can install it like this:\n",
        "\n",
        "```bash\n",
        "pip install python-dotenv==1.0.1\n",
        "```\n",
        "\n",
        "If you want to emulate the behavior of `^1.0.1` using `pip` (i.e., any version starting from `1.0.1` up to but not including the next major version, `2.0.0`), you could use:\n",
        "\n",
        "```bash\n",
        "pip install \"python-dotenv>=1.0.1,<2.0.0\"\n",
        "```\n",
        "\n",
        "This command tells `pip` to install a version of `python-dotenv` that is at least `1.0.1` but less than `2.0.0`. This will allow minor updates that are presumably backward compatible without crossing into a new major version that might introduce breaking changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
      "metadata": {
        "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
      "metadata": {
        "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80"
      },
      "source": [
        "#### Install LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa888f7-3718-4829-8645-30acb43db51f",
      "metadata": {
        "id": "6fa888f7-3718-4829-8645-30acb43db51f"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
      "metadata": {
        "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
      "metadata": {
        "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941"
      },
      "source": [
        "## Connect with an LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d5d5b71-b26a-4cd5-9765-019077a67141",
      "metadata": {
        "id": "4d5d5b71-b26a-4cd5-9765-019077a67141"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
      "metadata": {
        "id": "148df8e0-361d-4ddd-8709-af48fa1648d1"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
      "metadata": {
        "id": "a1998155-91de-4cbc-bc88-8d77beefb51b"
      },
      "source": [
        "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27",
      "metadata": {
        "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27"
      },
      "source": [
        "## LLM Model\n",
        "* The trend before the launch of chatGPT-4.\n",
        "* See LangChain documentation about LLM Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/llms/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92628f2-62e8-436c-92d4-e849de7744ad",
      "metadata": {
        "id": "e92628f2-62e8-436c-92d4-e849de7744ad"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llmModel = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdeaf300-5cb6-4df8-bcb9-1fe3c82e4b22",
      "metadata": {
        "id": "bdeaf300-5cb6-4df8-bcb9-1fe3c82e4b22"
      },
      "source": [
        "#### Invoke: all the text of the reponse is printed at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ade5fa-b487-4f32-bda7-0f9c41b096b5",
      "metadata": {
        "id": "93ade5fa-b487-4f32-bda7-0f9c41b096b5"
      },
      "outputs": [],
      "source": [
        "response = llmModel.invoke(\n",
        "    \"Tell me one fun fact about the Kennedy family.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a33285ef-4bb5-4fd6-9894-790c07a5468c",
      "metadata": {
        "id": "a33285ef-4bb5-4fd6-9894-790c07a5468c",
        "outputId": "d8e564d8-7221-4cf0-ee56-51c0577e9b03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n\\nOne fun fact about the Kennedy family is that they have their own personalized crest, which includes symbols such as a lion, an eagle, and a ship's wheel, representing courage, strength, and leadership.\""
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aa71043-4707-44b4-8d1f-8540d5d139b2",
      "metadata": {
        "id": "3aa71043-4707-44b4-8d1f-8540d5d139b2",
        "outputId": "1273b810-1eac-4a51-f4a3-0b5351f88f19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "One fun fact about the Kennedy family is that they have their own personalized crest, which includes symbols such as a lion, an eagle, and a ship's wheel, representing courage, strength, and leadership.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **📌 Notebook: \"1-connect-llm\" — Introduction & First Code Example**\n",
        "\n",
        "## **🎯 Overview**\n",
        "- **Notebook Purpose**: Demonstrate how to connect with an LLM (large language model) using LangChain.\n",
        "- **Primary Focus**: Show a simple code snippet for invoking a text-completion model (`OpenAI`), passing in a prompt, and receiving a response.\n",
        "\n",
        "---\n",
        "\n",
        "## **🔹 Key Points from the Summary**\n",
        "\n",
        "1. **LLM vs. Chat Models**  \n",
        "   - **Text-Completion Model**: Typically returns a single text block based on a prompt.  \n",
        "   - **Chat Model**: Handles multi-turn conversations with system, user, and AI roles.\n",
        "\n",
        "2. **Most Common Model**:  \n",
        "   - Since ChatGPT, **chat-based models** dominate in usage.  \n",
        "   - Nevertheless, text-completion models are still part of the LLM family.\n",
        "\n",
        "3. **Project Setup**:  \n",
        "   - **Environment**: Python 3.11.4, Poetry for dependencies.  \n",
        "   - **.env File**: Stores API keys and LangSmith tracking info.  \n",
        "   - **LangSmith Integration**: Monitors usage, logs, and costs at [smith.langchain.com](https://smith.langchain.com).\n",
        "\n",
        "---\n",
        "\n",
        "## **🔹 Code Snippet**\n",
        "\n",
        "    from langchain_openai import OpenAI\n",
        "\n",
        "    llmModel = OpenAI()\n",
        "\n",
        "    response = llmModel.invoke(\n",
        "        \"Tell me one fun fact about the Kennedy family.\"\n",
        "    )\n",
        "\n",
        "### **1️⃣ `from langchain_openai import OpenAI`**\n",
        "- **Imports** a LangChain integration class specifically for OpenAI’s text-completion models.\n",
        "- **Allows** you to configure settings like temperature, max tokens, or model name if desired.\n",
        "\n",
        "### **2️⃣ `llmModel = OpenAI()`**\n",
        "- **Instantiates** the `OpenAI` model object with default parameters.\n",
        "- By default, it expects an **`OPENAI_API_KEY`** in your environment variables (loaded via `.env`).\n",
        "\n",
        "### **3️⃣ `response = llmModel.invoke(...)`**\n",
        "- **Sends** the string prompt to the model: `\"Tell me one fun fact about the Kennedy family.\"`\n",
        "- **Returns** a single text response (e.g., some historical tidbit about the Kennedys).\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Why This Matters**\n",
        "\n",
        "1. **First Contact with the LLM**  \n",
        "   - Proves you can **successfully** connect to OpenAI’s text-completion endpoint.\n",
        "2. **Modular Approach**  \n",
        "   - `OpenAI()` can be swapped for **other LLM backends** if you choose.\n",
        "3. **LangSmith Tracking**  \n",
        "   - If properly configured, you can **monitor** your usage and costs, especially helpful for scaling apps.\n",
        "\n",
        "---\n",
        "\n",
        "## **💡 Example Usage**\n",
        "\n",
        "- Prompt: `\"Tell me one fun fact about the Kennedy family.\"`  \n",
        "- **Potential Response**: “John F. Kennedy was the first U.S. president to... (interesting fact).”  \n",
        "- The LLM might discuss trivia about JFK’s early life, the family’s political influence, or other unique tidbits.\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Key Takeaways**\n",
        "\n",
        "1. **Simple Prompt**: Demonstrates how to send a plain English request and get a straightforward answer.\n",
        "2. **Flexibility**: Parameters like `temperature=0.7` can be added to control creativity.\n",
        "3. **Foundation**: Sets the stage for more advanced operations (e.g., chaining prompts, chat-based flows).\n",
        "\n",
        "> *Think of this as your “Hello World” for LLM integration—a single prompt that confirms everything’s wired up and ready for deeper exploration!*\n"
      ],
      "metadata": {
        "id": "DMJHxE38_sQw"
      },
      "id": "DMJHxE38_sQw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "158aa111-9167-444c-9b10-90b095eac390",
      "metadata": {
        "id": "158aa111-9167-444c-9b10-90b095eac390",
        "outputId": "6a672783-6c19-455a-f6a4-77bdcff04119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "One fun fact about the Kennedy family is that President John F. Kennedy's favorite meal was New England fish chowder. He often requested it for meals at the White House and even had a special recipe created just for him."
          ]
        }
      ],
      "source": [
        "# Streaming: printing one chunk of text at a time\n",
        "for chunk in llmModel.stream(\n",
        "    \"Tell me one fun fact about the Kennedy family.\"\n",
        "):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **📌 Step 2: Streaming Model Responses**\n",
        "\n",
        "## **🎯 Objective**\n",
        "- **Showcase** how to retrieve an LLM’s response in **real-time** chunks (streaming).\n",
        "- **Improve** user experience by displaying partial output as it’s generated rather than waiting for the entire answer.\n",
        "\n",
        "---\n",
        "\n",
        "## **🔹 Code Snippet**\n",
        "    # Streaming: printing one chunk of text at a time\n",
        "    for chunk in llmModel.stream(\n",
        "        \"Tell me one fun fact about the Kennedy family.\"\n",
        "    ):\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "\n",
        "### **1️⃣ `.stream(...)` Method**\n",
        "- **Initiates** a real-time connection to the model.\n",
        "- **Returns** text in incremental chunks, which can be printed as they arrive.\n",
        "\n",
        "### **2️⃣ `for chunk in llmModel.stream(...)` Loop**\n",
        "- **Iterates** through each chunk of generated text.\n",
        "- **Output** is shown piece by piece, making it appear more conversational or responsive.\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Why Stream Responses?**\n",
        "1. **Faster User Feedback**: Users immediately see partial results.\n",
        "2. **Interactive Feel**: Simulates a typing effect or “live conversation.”\n",
        "3. **Long Outputs**: Especially helpful when generating **lengthy** content, so users don’t wait for the entire text to finish.\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Key Takeaways**\n",
        "1. **Streaming** is an excellent technique for creating a **dynamic** user interface.  \n",
        "2. The `.stream(...)` method ensures partial data is processed as it becomes available.  \n",
        "3. This approach is particularly useful for **chat-like** or **real-time** applications.\n",
        "\n",
        "> *Imagine `.stream()` as eavesdropping on the LLM’s thought process—text arrives bit by bit, giving your users a more immediate and engaging experience.*\n"
      ],
      "metadata": {
        "id": "zpQg1M4D_8rl"
      },
      "id": "zpQg1M4D_8rl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac12473a-1ffd-4a22-852a-92e1e6374914",
      "metadata": {
        "id": "ac12473a-1ffd-4a22-852a-92e1e6374914"
      },
      "outputs": [],
      "source": [
        "# Temperature: more or less creativity\n",
        "creativeLlmModel = OpenAI(temperature=0.9)\n",
        "response = llmModel.invoke(\n",
        "    \"Write a short 5 line poem about JFK\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "679c1220-13ed-4251-ba4b-f8f1ba350f2d",
      "metadata": {
        "id": "679c1220-13ed-4251-ba4b-f8f1ba350f2d",
        "outputId": "0b616903-3d7d-4986-df72-a2edbc774137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "A leader with grace and charm,\n",
            "His words inspired and disarmed,\n",
            "JFK, a symbol of hope,\n",
            "With a vision beyond the scope,\n",
            "Forever remembered, his legacy will never depart.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **📌 Step 3: Controlling Model Creativity with `temperature`**\n",
        "\n",
        "## **🎯 Objective**\n",
        "- **Demonstrate** how to adjust the LLM’s **creativity** by setting a `temperature` parameter.\n",
        "- **Show** how it affects the output by providing a prompt that encourages a more imaginative response.\n",
        "\n",
        "---\n",
        "\n",
        "## **🔹 Code Snippet**\n",
        "\n",
        "    # Temperature: more or less creativity\n",
        "    creativeLlmModel = OpenAI(temperature=0.9)\n",
        "    response = llmModel.invoke(\n",
        "        \"Write a short 5 line poem about JFK\"\n",
        "    )\n",
        "\n",
        "### **1️⃣ `OpenAI(temperature=0.9)`**\n",
        "- **Creates** a new model instance (`creativeLlmModel`) with a higher temperature.\n",
        "- **Higher temperature** → More randomness, novelty, and figurative language in the response.\n",
        "\n",
        "### **2️⃣ `response = llmModel.invoke(\"...\")`**\n",
        "- **Generates** a text completion for the poem about JFK.\n",
        "- Note that in the snippet, `llmModel` (not `creativeLlmModel`) is actually invoked. If you want to use the higher creativity model, replace `llmModel` with `creativeLlmModel`.\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Why Adjust Temperature?**\n",
        "1. **Enhanced Creativity**  \n",
        "   - Higher temperature can produce more **original** or **artistic** text.  \n",
        "2. **Predictable Responses**  \n",
        "   - Lower temperature yields **consistent, factual** answers, better for straightforward tasks.  \n",
        "3. **Fine-Tuning Output**  \n",
        "   - Experimenting with temperature helps **balance** creativity vs. reliability, depending on your application.\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Key Takeaways**\n",
        "1. **Temperature** is crucial in **prompt engineering**: high for expressive tasks, low for accuracy-focused tasks.  \n",
        "2. The snippet shows **two** LLM objects—`creativeLlmModel` and `llmModel`. If you want the creative output, **invoke** the `creativeLlmModel`.  \n",
        "3. Poetry or narrative prompts benefit from increased temperature, while Q&A or data extraction usually lean lower.\n",
        "\n",
        "> *Think of `temperature` like the LLM’s imagination dial—turn it up to let the model dream, dial it down to keep it on-track.*\n"
      ],
      "metadata": {
        "id": "e37auZhhAaqa"
      },
      "id": "e37auZhhAaqa"
    },
    {
      "cell_type": "markdown",
      "id": "31346a0c-ee9c-4a80-8e8a-85ef682df7c3",
      "metadata": {
        "id": "31346a0c-ee9c-4a80-8e8a-85ef682df7c3"
      },
      "source": [
        "# Chat Model\n",
        "* The general trend after the launch of chatGPT-4.\n",
        "    * Frequently known as \"Chatbot\".\n",
        "    * Conversation between Human and AI.\n",
        "    * Can have a system prompt defining the tone or the role of the AI.\n",
        "* See LangChain documentation about Chat Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/chat/).\n",
        "* By default we will work with ChatOpenAI. See [here](https://python.langchain.com/v0.1/docs/integrations/chat/openai/) the LangChain documentation page about it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7",
      "metadata": {
        "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eaa967e-067f-4574-8cbc-b1e5b7956fa2",
      "metadata": {
        "id": "0eaa967e-067f-4574-8cbc-b1e5b7956fa2"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are an historian expert in the Kennedy family.\"),\n",
        "    (\"human\", \"Tell me one curious thing about JFK.\"),\n",
        "]\n",
        "response = chatModel.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **📌 Step 4: Using a Chat Model (`ChatOpenAI`) for Role-Based Conversations**\n",
        "\n",
        "## **🎯 Objective**\n",
        "- **Demonstrate** how to create a conversation using LangChain’s `ChatOpenAI`.\n",
        "- **Highlight** the system prompt’s role in setting the AI’s persona or expertise.\n",
        "\n",
        "---\n",
        "\n",
        "## **🔹 Code Snippet**\n",
        "\n",
        "    from langchain_openai import ChatOpenAI\n",
        "\n",
        "    chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "    messages = [\n",
        "        (\"system\", \"You are an historian expert in the Kennedy family.\"),\n",
        "        (\"human\", \"Tell me one curious thing about JFK.\"),\n",
        "    ]\n",
        "\n",
        "    response = chatModel.invoke(messages)\n",
        "\n",
        "---\n",
        "\n",
        "## **1️⃣ `ChatOpenAI(model=\"gpt-3.5-turbo-0125\")`**\n",
        "- **Initializes** a chat-based LLM tied to a specific GPT-3.5 variant (`0125` release).\n",
        "- **Chat context**: Built to handle system, user, and AI roles, reflecting how ChatGPT and similar chat models operate.\n",
        "\n",
        "### **System Role**\n",
        "- **Defines** the overarching instructions: “You are an historian expert in the Kennedy family.”\n",
        "- **Effect**: The model aims to provide historically grounded responses about JFK.\n",
        "\n",
        "### **Human Role**\n",
        "- **Represents** the user’s question: “Tell me one curious thing about JFK.”\n",
        "\n",
        "---\n",
        "\n",
        "## **2️⃣ `chatModel.invoke(messages)`**\n",
        "- **Passes** the role-based message list to the chat model.\n",
        "- **Returns** a single message from the AI role, typically containing the answer or continuation of the conversation.\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Key Differences from Text-Completion Models**\n",
        "1. **Role-Based**: Chat models expect input broken down by **system**, **user**, and possibly **assistant** messages.\n",
        "2. **Context Preservation**: Each message in `messages` helps the model maintain a conversation flow.\n",
        "3. **Flexible Interaction**: You can keep appending new user messages to the same conversation for multi-turn dialogues.\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Key Takeaways**\n",
        "1. **System Prompts** shape the AI’s persona and domain knowledge.  \n",
        "2. **Chat Models** are suited for multi-turn conversations, often with a more **conversational** or **contextual** tone.  \n",
        "3. **Model Version** (`gpt-3.5-turbo-0125`) ensures consistent output, as different releases may have subtle variations in behavior.\n",
        "\n",
        "> *View `ChatOpenAI` as a conversation manager—each role-based message adds context, guiding the model’s next reply.*\n"
      ],
      "metadata": {
        "id": "Nnq6DnKJAr1D"
      },
      "id": "Nnq6DnKJAr1D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f567f5a-c4ea-4234-84e6-422bf6104589",
      "metadata": {
        "id": "2f567f5a-c4ea-4234-84e6-422bf6104589",
        "outputId": "c5c2f740-a022-4342-8a48-951b2f4691cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"One curious thing about JFK is that he was a collector of unique and eclectic items. He had a fascination with history and art, and his collection included everything from ship models and scrimshaw to original manuscripts and rare books. JFK's love of collecting even extended to quirky items like coconut shells carved with faces that he displayed in the Oval Office. This hobby provided a glimpse into his personal interests and served as a reflection of his intellectual curiosity.\", response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 29, 'total_tokens': 116}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b2f90721-52fe-4482-989a-bc75d6f3e7fd-0', usage_metadata={'input_tokens': 29, 'output_tokens': 87, 'total_tokens': 116})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a6abd4-a072-47ea-ae60-7959def9b602",
      "metadata": {
        "id": "19a6abd4-a072-47ea-ae60-7959def9b602",
        "outputId": "3989bffc-c6d6-4770-df91-6dd859e4f4a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"One curious thing about JFK is that he was a collector of unique and eclectic items. He had a fascination with history and art, and his collection included everything from ship models and scrimshaw to original manuscripts and rare books. JFK's love of collecting even extended to quirky items like coconut shells carved with faces that he displayed in the Oval Office. This hobby provided a glimpse into his personal interests and served as a reflection of his intellectual curiosity.\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47927ac1-13bc-40eb-9917-9d39418f0f7d",
      "metadata": {
        "id": "47927ac1-13bc-40eb-9917-9d39418f0f7d",
        "outputId": "d59a15ae-8209-41dc-d57e-6c2b353228cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'token_usage': {'completion_tokens': 87,\n",
              "  'prompt_tokens': 29,\n",
              "  'total_tokens': 116},\n",
              " 'model_name': 'gpt-3.5-turbo-0125',\n",
              " 'system_fingerprint': None,\n",
              " 'finish_reason': 'stop',\n",
              " 'logprobs': None}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.response_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **📌 Step 5: Inspecting Response Metadata**\n",
        "\n",
        "## **🎯 Objective**\n",
        "- **Examine** additional information returned by the LLM after it processes a request.\n",
        "- **Understand** how token usage and model details are encapsulated in `response.response_metadata`.\n",
        "\n",
        "---\n",
        "\n",
        "## **🔹 Example Metadata**\n",
        "\n",
        "    {\n",
        "      'token_usage': {\n",
        "        'completion_tokens': 87,\n",
        "        'prompt_tokens': 29,\n",
        "        'total_tokens': 116\n",
        "      },\n",
        "      'model_name': 'gpt-3.5-turbo-0125',\n",
        "      'system_fingerprint': None,\n",
        "      'finish_reason': 'stop',\n",
        "      'logprobs': None\n",
        "    }\n",
        "\n",
        "### **1️⃣ `token_usage`**\n",
        "- **`completion_tokens`**: Number of tokens the model used to generate its response.  \n",
        "- **`prompt_tokens`**: Number of tokens used in the input prompt (including system, user messages, etc.).  \n",
        "- **`total_tokens`**: The sum of both prompt and completion tokens. Useful for **tracking usage** and potential costs.\n",
        "\n",
        "### **2️⃣ `model_name`**\n",
        "- Indicates the **specific model** that handled the request.  \n",
        "- For example, `\"gpt-3.5-turbo-0125\"` confirms which snapshot of GPT-3.5 was used.\n",
        "\n",
        "### **3️⃣ `system_fingerprint`**\n",
        "- **Nullable** field that might store a **hash** or **identifier** for system messages or instructions.  \n",
        "- In this example, it’s `None`.\n",
        "\n",
        "### **4️⃣ `finish_reason`**\n",
        "- **Shows** why the model stopped its generation. Common reasons:\n",
        "  - `stop`: The model reached a stopping point naturally.  \n",
        "  - `length`: The model hit a token limit.  \n",
        "  - `max_tokens`: The response ran out of allowable tokens.\n",
        "\n",
        "### **5️⃣ `logprobs`**\n",
        "- **Log probabilities** of the generated tokens—helpful for advanced analysis or debugging.  \n",
        "- Often returned as `None` by default, unless specifically requested via certain parameters.\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Key Takeaways**\n",
        "1. **Token Usage** is crucial for monitoring **API costs** and optimizing prompts to stay within usage limits.  \n",
        "2. **Model Name** ensures you know exactly which version or snapshot of GPT processed your request.  \n",
        "3. Additional fields like **`finish_reason`** provide insight into how and why the model concluded its response.\n",
        "\n",
        "> *Think of `response_metadata` as the LLM’s receipt—everything you need to know about the transaction is right here, from the tokens consumed to the model’s finishing reasons.*\n"
      ],
      "metadata": {
        "id": "yO1tBsrMBCcQ"
      },
      "id": "yO1tBsrMBCcQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7847143f-3179-423e-b499-a6e494967741",
      "metadata": {
        "id": "7847143f-3179-423e-b499-a6e494967741",
        "outputId": "623c7565-c828-47ad-96f5-a1292310cef4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': 'AIMessage',\n",
              " 'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
              " 'type': 'object',\n",
              " 'properties': {'content': {'title': 'Content',\n",
              "   'anyOf': [{'type': 'string'},\n",
              "    {'type': 'array',\n",
              "     'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
              "  'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
              "  'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
              "  'type': {'title': 'Type', 'default': 'ai', 'enum': ['ai'], 'type': 'string'},\n",
              "  'name': {'title': 'Name', 'type': 'string'},\n",
              "  'id': {'title': 'Id', 'type': 'string'},\n",
              "  'example': {'title': 'Example', 'default': False, 'type': 'boolean'},\n",
              "  'tool_calls': {'title': 'Tool Calls',\n",
              "   'default': [],\n",
              "   'type': 'array',\n",
              "   'items': {'$ref': '#/definitions/ToolCall'}},\n",
              "  'invalid_tool_calls': {'title': 'Invalid Tool Calls',\n",
              "   'default': [],\n",
              "   'type': 'array',\n",
              "   'items': {'$ref': '#/definitions/InvalidToolCall'}},\n",
              "  'usage_metadata': {'$ref': '#/definitions/UsageMetadata'}},\n",
              " 'required': ['content'],\n",
              " 'definitions': {'ToolCall': {'title': 'ToolCall',\n",
              "   'type': 'object',\n",
              "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
              "    'args': {'title': 'Args', 'type': 'object'},\n",
              "    'id': {'title': 'Id', 'type': 'string'},\n",
              "    'type': {'title': 'Type', 'enum': ['tool_call'], 'type': 'string'}},\n",
              "   'required': ['name', 'args', 'id']},\n",
              "  'InvalidToolCall': {'title': 'InvalidToolCall',\n",
              "   'type': 'object',\n",
              "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
              "    'args': {'title': 'Args', 'type': 'string'},\n",
              "    'id': {'title': 'Id', 'type': 'string'},\n",
              "    'error': {'title': 'Error', 'type': 'string'},\n",
              "    'type': {'title': 'Type',\n",
              "     'enum': ['invalid_tool_call'],\n",
              "     'type': 'string'}},\n",
              "   'required': ['name', 'args', 'id', 'error']},\n",
              "  'UsageMetadata': {'title': 'UsageMetadata',\n",
              "   'type': 'object',\n",
              "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
              "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
              "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'}},\n",
              "   'required': ['input_tokens', 'output_tokens', 'total_tokens']}}}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.schema()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **📌 Step 6: Exploring the Schema of an AIMessage**\n",
        "\n",
        "## **🎯 Objective**\n",
        "- **Understand** the JSON schema returned by the `response.schema()` call.\n",
        "- **Identify** how LangChain structures AI messages, including content, metadata, and other optional fields.\n",
        "\n",
        "---\n",
        "\n",
        "## **1️⃣ Core Properties**\n",
        "\n",
        "- **`content`**  \n",
        "  - Main text from the AI. Can be a **string** or a **list** (e.g., chunks or structured data).\n",
        "- **`additional_kwargs`**  \n",
        "  - Freeform dictionary for extra parameters or context not explicitly captured in the main schema.\n",
        "- **`response_metadata`**  \n",
        "  - Model name, token usage, or other metadata related to the response.\n",
        "- **`tool_calls` / `invalid_tool_calls`**  \n",
        "  - Specifies any tools the AI invoked (or tried to invoke incorrectly).\n",
        "- **`usage_metadata`**  \n",
        "  - Contains input/output/total token counts.\n",
        "\n",
        "---\n",
        "\n",
        "## **2️⃣ Usage & Importance**\n",
        "\n",
        "1. **Validation**  \n",
        "   - Ensures any AI message meets the required structure (`content` is mandatory).\n",
        "2. **Consistency**  \n",
        "   - Standardizes how messages, metadata, and tool interactions are stored across LangChain.\n",
        "3. **Debugging & Analysis**  \n",
        "   - Fields like `usage_metadata` and `response_metadata` help **monitor** costs and understand why a response was generated (e.g., was a tool call attempted?).\n",
        "\n",
        "---\n",
        "\n",
        "## **3️⃣ Applications**\n",
        "\n",
        "- **Tool Integration**  \n",
        "  - AI messages that include `tool_calls` might pass commands to external tools (e.g., web searches, calculators).\n",
        "- **Conversation Flows**  \n",
        "  - Tracking `invalid_tool_calls` helps refine prompts if the AI repeatedly attempts calls that fail.\n",
        "- **Usage Tracking**  \n",
        "  - `usage_metadata` is critical for monitoring token consumption, especially for cost management in production environments.\n",
        "\n",
        "---\n",
        "\n",
        "## **📌 Key Takeaways**\n",
        "1. **`AIMessage` Schema** outlines how LangChain **structures** AI responses, including content, metadata, and tool interactions.  \n",
        "2. **Required Fields**: At minimum, every AI message must include `content`.  \n",
        "3. **Debug Tools**: The schema reveals fields for usage stats (`usage_metadata`), which tie into cost and performance analytics.\n",
        "\n",
        "> *Think of `response.schema()` as a blueprint, ensuring every AI-generated message has the same building blocks—making your app more robust and easier to maintain.*\n"
      ],
      "metadata": {
        "id": "WtHd_xycBS30"
      },
      "id": "WtHd_xycBS30"
    },
    {
      "cell_type": "markdown",
      "id": "54631efb-dbe2-4bd7-b588-324f19a28b2c",
      "metadata": {
        "id": "54631efb-dbe2-4bd7-b588-324f19a28b2c"
      },
      "source": [
        "# Classic Method (Connect LLM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20ba943-ce6b-45e2-9dbb-351c48421069",
      "metadata": {
        "id": "a20ba943-ce6b-45e2-9dbb-351c48421069"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b879a5f4-2250-4a70-b065-7796c4fe9f06",
      "metadata": {
        "id": "b879a5f4-2250-4a70-b065-7796c4fe9f06"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are an historian expert on the Kennedy Family.\"),\n",
        "    HumanMessage(content=\"How many children had Joseph P. Kennedy?\"),\n",
        "]\n",
        "\n",
        "response = chatModel.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f9b26b-e7e1-4613-8e3e-0c81f8b54da6",
      "metadata": {
        "id": "78f9b26b-e7e1-4613-8e3e-0c81f8b54da6",
        "outputId": "252f80c2-c53b-4ea8-a1db-14150857fd18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Joseph P. Kennedy and his wife Rose Fitzgerald Kennedy had nine children. Their children were Joseph Jr., John F. (JFK), Rosemary, Kathleen, Eunice, Patricia, Robert F. (Bobby), Jean, and Edward M. (Ted).', response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 30, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-63964416-759f-461a-aecb-789b336ce0c1-0', usage_metadata={'input_tokens': 30, 'output_tokens': 54, 'total_tokens': 84})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e41204f4-0cec-4039-a18d-2a25b9dd5efd",
      "metadata": {
        "id": "e41204f4-0cec-4039-a18d-2a25b9dd5efd"
      },
      "source": [
        "#### Streaming:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d9f1d0-bba8-4885-9ac1-01fc273732b1",
      "metadata": {
        "id": "83d9f1d0-bba8-4885-9ac1-01fc273732b1",
        "outputId": "35e2e83f-0c04-4ac4-fb4b-3b0cc61bd79f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joseph P. Kennedy and his wife Rose Kennedy had nine children: Joseph Jr., John F. (JFK), Rosemary, Kathleen, Eunice, Patricia, Robert (Bobby), Jean, and Edward (Ted)."
          ]
        }
      ],
      "source": [
        "for chunk in chatModel.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acd276b9-494a-47c2-8455-2ff33fc4f221",
      "metadata": {
        "id": "acd276b9-494a-47c2-8455-2ff33fc4f221"
      },
      "source": [
        "#### Another old way, similar results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bc0210b-27e0-41ed-8ad8-ff1c75c9bef4",
      "metadata": {
        "id": "4bc0210b-27e0-41ed-8ad8-ff1c75c9bef4"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are expert {profession} in {topic}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chatModel\n",
        "\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"profession\": \"Historian\",\n",
        "        \"topic\": \"Kennedy Family\",\n",
        "        \"input\": \"Tell me one fun fact about JFK.\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc436d53-b372-43eb-ac00-ee39c9fa491a",
      "metadata": {
        "id": "cc436d53-b372-43eb-ac00-ee39c9fa491a",
        "outputId": "bc6b8ac0-d06d-426c-dfc0-51b07b5078bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='One fun fact about JFK is that he was the first president to hold a press conference that was broadcast live on television. This event took place on January 25, 1961, and it allowed the American public to see and hear their president in real-time, marking a significant shift in how political communication was conducted.', response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 28, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3e6f4f2c-a7fe-4220-925e-1e3a6954cc57-0', usage_metadata={'input_tokens': 28, 'output_tokens': 64, 'total_tokens': 92})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444",
      "metadata": {
        "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444"
      },
      "source": [
        "## How to execute the code from Visual Studio Code\n",
        "* In Visual Studio Code, see the file 001-connect-llms.py\n",
        "* In terminal, make sure you are in the directory of the file and run:\n",
        "    * python 001-connect-llm.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069",
      "metadata": {
        "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}