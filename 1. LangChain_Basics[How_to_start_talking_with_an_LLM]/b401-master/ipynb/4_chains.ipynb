{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **4-chains**\n",
        "\n",
        "## **ðŸ¤– Introduction**\n",
        "- **Chains** allow you to perform several actions in a **specific order**, enabling complex workflows where the output of one step can become the input to another.\n",
        "\n",
        "## **âš™ï¸ Setup**\n",
        "1. **Clone** or **Download** the GitHub repository to your machine.  \n",
        "2. In **terminal**:\n",
        "   ```\n",
        "   cd project_name\n",
        "   pyenv local 3.11.4\n",
        "   poetry install\n",
        "   poetry shell\n",
        "   ```\n",
        "3. To open the notebook with **Jupyter Notebooks**:\n",
        "   ```\n",
        "   jupyter lab\n",
        "   ```\n",
        "   - Go to the **notebooks folder** and open the `004-chains.ipynb` file.\n",
        "\n",
        "4. **View Code** in Visual Studio Code or any other editor:\n",
        "   - Locate and open `004-chains.py`.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ” Create Your `.env` File**\n",
        "- A file named **`.env.example`** is included in the repository.\n",
        "- Rename it to **`.env`** and add your **confidential API keys**:\n",
        "  ```\n",
        "  OPENAI_API_KEY=your_openai_api_key\n",
        "  LANGCHAIN_TRACING_V2=true\n",
        "  LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
        "  LANGCHAIN_API_KEY=your_langchain_api_key\n",
        "  LANGCHAIN_PROJECT=your_project_name\n",
        "  ```\n",
        "- This LangSmith project is **`004-chains`**.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Š Track Operations**\n",
        "- From now on, **monitor usage and costs** for this project in **LangSmith**:\n",
        "  ```\n",
        "  smith.langchain.com\n",
        "  ```\n",
        "\n",
        "> **ðŸ’¡ Pro Tip**: By chaining multiple steps together (e.g., prompts, data transformations, or tool usage), you can **automate** more sophisticated logic and integrate multiple LLM calls within a single pipeline.\n"
      ],
      "metadata": {
        "id": "zC0g8GiA3lfZ"
      },
      "id": "zC0g8GiA3lfZ"
    },
    {
      "cell_type": "markdown",
      "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291",
      "metadata": {
        "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291"
      },
      "source": [
        "## Connect with the .env file located in the same directory of this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a",
      "metadata": {
        "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a10870-432e-4818-aa5e-6be24c579d39",
      "metadata": {
        "id": "e9a10870-432e-4818-aa5e-6be24c579d39"
      },
      "outputs": [],
      "source": [
        "#pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
      "metadata": {
        "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
      "metadata": {
        "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80"
      },
      "source": [
        "#### Install LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa888f7-3718-4829-8645-30acb43db51f",
      "metadata": {
        "id": "6fa888f7-3718-4829-8645-30acb43db51f"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
      "metadata": {
        "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
      "metadata": {
        "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941"
      },
      "source": [
        "## Connect with an LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d5d5b71-b26a-4cd5-9765-019077a67141",
      "metadata": {
        "id": "4d5d5b71-b26a-4cd5-9765-019077a67141"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
      "metadata": {
        "id": "148df8e0-361d-4ddd-8709-af48fa1648d1"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
      "metadata": {
        "id": "a1998155-91de-4cbc-bc88-8d77beefb51b"
      },
      "source": [
        "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27",
      "metadata": {
        "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27"
      },
      "source": [
        "## LLM Model\n",
        "* The trend before the launch of chatGPT-4.\n",
        "* See LangChain documentation about LLM Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/llms/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92628f2-62e8-436c-92d4-e849de7744ad",
      "metadata": {
        "id": "e92628f2-62e8-436c-92d4-e849de7744ad"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llmModel = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31346a0c-ee9c-4a80-8e8a-85ef682df7c3",
      "metadata": {
        "id": "31346a0c-ee9c-4a80-8e8a-85ef682df7c3"
      },
      "source": [
        "## Chat Model\n",
        "* The general trend after the launch of chatGPT-4.\n",
        "    * Frequently known as \"Chatbot\".\n",
        "    * Conversation between Human and AI.\n",
        "    * Can have a system prompt defining the tone or the role of the AI.\n",
        "* See LangChain documentation about Chat Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/chat/).\n",
        "* By default we will work with ChatOpenAI. See [here](https://python.langchain.com/v0.1/docs/integrations/chat/openai/) the LangChain documentation page about it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7",
      "metadata": {
        "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5383c7e-7e62-46d0-8bef-17ff45a88495",
      "metadata": {
        "id": "a5383c7e-7e62-46d0-8bef-17ff45a88495",
        "outputId": "48427fb3-eec5-4885-895d-d34b703839b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nDuring John F. Kennedy\\'s presidency, his brother Robert F. Kennedy, who served as Attorney General, had a unique way of dealing with stress. He would often go down to the White House kitchen and bake pies. This became a well-known secret among the White House staff, who would often receive freshly baked pies from RFK himself.\\n\\nOne day, while baking a lemon meringue pie, RFK accidentally spilled some of the filling on his shirt. Not wanting to go back to work with a stained shirt, he quickly took it off and asked one of the kitchen staff to wash it for him. The staff member, not realizing whose shirt it was, took it home to his wife to wash.\\n\\nThe next day, the staff member\\'s wife was watching the news and saw RFK giving a press conference, wearing the same shirt that she had just washed. She immediately recognized it as her husband\\'s work and called the White House to inform them. The Kennedy family found the situation amusing and invited the staff member and his wife to the White House for a private tour.\\n\\nFrom then on, RFK\\'s secret baking sessions became a regular occurrence and the staff member\\'s wife became the official \"pie washer\" for the Kennedy family. It was a special and unique bond'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Define a template that incorporates placeholders for 'adjective' and 'topic'\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} story about {topic}.\"\n",
        ")\n",
        "\n",
        "# Fill in the template with specific values\n",
        "llmModelPrompt = prompt_template.format(\n",
        "    adjective=\"curious\",\n",
        "    topic=\"the Kennedy family\"\n",
        ")\n",
        "\n",
        "# Invoke the LLM with the resulting prompt\n",
        "llmModel.invoke(llmModelPrompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ“ What Are Prompts?**\n",
        "- **Prompts** are the **input** you give to an LLM, shaping how it generates text.\n",
        "- **LangChain** provides utilities like `PromptTemplate` to create **parameterized** prompts.\n",
        "\n",
        "## **âš™ï¸ Step-by-Step Breakdown**\n",
        "1. **`PromptTemplate.from_template(...)`**  \n",
        "   - Constructs a template string with placeholders (`{adjective}`, `{topic}`).\n",
        "   - Ideal for reusing the same structure while changing specific fields.\n",
        "\n",
        "2. **`prompt_template.format(...)`**  \n",
        "   - Substitutes real values (e.g., `\"curious\"`, `\"the Kennedy family\"`) into those placeholders.\n",
        "   - Results in a final string: â€œTell me a curious story about the Kennedy family.â€\n",
        "\n",
        "3. **`llmModel.invoke(...)`**  \n",
        "   - Sends the formatted prompt to the **LLM**, which returns a tailored response.\n",
        "   - The model can produce a short narrative or anecdote based on your instructions.\n",
        "\n",
        "## **ðŸ’¡ Why Use Prompt Templates?**\n",
        "- **Scalability**: Quickly adapt your prompt for different topics or styles without rewriting code.\n",
        "- **Consistency**: Maintain a uniform structure across various prompts in larger applications.\n",
        "- **Modularity**: Keep your instructions, few-shot examples, and context in a single template file.\n",
        "\n",
        "> **ðŸ”Ž Pro Tip**: You can combine multiple placeholders for more nuanced prompts (e.g., add a `tone` or `length` parameter). By doing so, you ensure your LLM remains flexible yet focused on specific queries."
      ],
      "metadata": {
        "id": "H4WlPRjd37MD"
      },
      "id": "H4WlPRjd37MD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb7dbcd-524c-4acc-9b65-0b209a170ad7",
      "metadata": {
        "id": "beb7dbcd-524c-4acc-9b65-0b209a170ad7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define a chat prompt template with various role messages\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
        "        (\"human\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
        "        (\"ai\", \"Sure!\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Substitute placeholders with actual values\n",
        "messages = chat_template.format_messages(\n",
        "    profession=\"Historian\",\n",
        "    topic=\"The Kennedy family\",\n",
        "    user_input=\"How many grandchildren had Joseph P. Kennedy?\"\n",
        ")\n",
        "\n",
        "# Invoke the chat model with the constructed conversation\n",
        "response = chatModel.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ”¹ ChatPromptTemplate Basics**\n",
        "- **Conversation Flow**: Each tuple (`(role, content)`) establishes **who** is speaking (system, human, ai) and **what** they say.  \n",
        "- **Parameterization**: Placeholders (`{profession}`, `{topic}`, `{user_input}`) let you **dynamically** inject context.\n",
        "\n",
        "## **ðŸ”Ž Step-by-Step**\n",
        "1. **System Role**  \n",
        "   - `\"You are an {profession} expert on {topic}.\"`  \n",
        "   - Sets the **background** or **persona** the model should assume (e.g., Historian for the Kennedy family).\n",
        "\n",
        "2. **Human Role**  \n",
        "   - **Initial Greeting**: â€œHello, Mr. {profession}â€¦â€â€”mimics an interactive conversation.  \n",
        "   - **User Question**: â€œHow many grandchildren had Joseph P. Kennedy?â€ is inserted via `user_input`.\n",
        "\n",
        "3. **AI Role**  \n",
        "   - **â€œSure!â€** demonstrates a placeholder response from the AI. In real usage, the modelâ€™s next turn will build upon this context.\n",
        "\n",
        "4. **Formatted Messages**  \n",
        "   - **`chat_template.format_messages(...)`** populates each placeholder with real valuesâ€”â€œHistorian,â€ â€œThe Kennedy family,â€ etc.  \n",
        "   - Produces a **ready-to-send** conversation for the LLM.\n",
        "\n",
        "5. **Model Invocation**  \n",
        "   - **`chatModel.invoke(messages)`** sends this array of messages to the underlying chat model, which uses role-based context to generate a response.\n",
        "\n",
        "## **ðŸ¤” Why Use Role-Based Chat?**\n",
        "- **Context Retention**: System instructions guide the AIâ€™s overall style and knowledge domain.  \n",
        "- **Multi-Turn Interactions**: Additional **human** or **ai** messages can extend conversations beyond a single Q&A.  \n",
        "- **Clarity**: Splitting conversation roles keeps prompts structured and easier to **modify** or **debug**.\n",
        "\n",
        "## **ðŸ’¡ Pro Tips**\n",
        "- **Add More Examples**: Provide short Q&A pairs in the conversation to show the model how you want it to respond (few-shot learning).  \n",
        "- **Fine-Tune Tone**: Adjust system messages to set a more formal, casual, or playful style.  \n",
        "- **Chain More Steps**: The result here could feed into another chain step for summarizing, analyzing, or storing the response for further processing.\n",
        "\n",
        "> **âš™ï¸ Ready for Scalability**: By using templates and placeholders, you can swiftly adapt this conversation format for **other historical figures**, **new queries**, or different model roles, all without rewriting your entire prompt logic."
      ],
      "metadata": {
        "id": "JWOGGbCV4beo"
      },
      "id": "JWOGGbCV4beo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7585be48-9a19-43ab-a275-8ef6a04246a7",
      "metadata": {
        "id": "7585be48-9a19-43ab-a275-8ef6a04246a7",
        "outputId": "34c753f9-e078-4763-e379-37a45ef61c72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Joseph P. Kennedy, the patriarch of the Kennedy family, had a total of 34 grandchildren. These grandchildren are the descendants of his nine children, including President John F. Kennedy, Senator Robert F. Kennedy, and other members of the Kennedy clan.', response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 55, 'total_tokens': 106}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0938d507-1e35-4586-b18d-0b4aa08cb7ce-0', usage_metadata={'input_tokens': 55, 'output_tokens': 51, 'total_tokens': 106})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1373ca0b-342f-4ac6-a697-5e2ad30bfa8c",
      "metadata": {
        "id": "1373ca0b-342f-4ac6-a697-5e2ad30bfa8c",
        "outputId": "5b2ef431-ef8f-44e3-ded3-c3cfaf8b4587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Joseph P. Kennedy, the patriarch of the Kennedy family, had a total of 34 grandchildren. These grandchildren are the descendants of his nine children, including President John F. Kennedy, Senator Robert F. Kennedy, and other members of the Kennedy clan.' response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 55, 'total_tokens': 106}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-0938d507-1e35-4586-b18d-0b4aa08cb7ce-0' usage_metadata={'input_tokens': 55, 'output_tokens': 51, 'total_tokens': 106}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f6a5e25-e177-4d7a-ba29-6834a6a8152b",
      "metadata": {
        "id": "6f6a5e25-e177-4d7a-ba29-6834a6a8152b",
        "outputId": "19f00765-1cbd-45db-dd14-e14a0b8283f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joseph P. Kennedy, the patriarch of the Kennedy family, had a total of 34 grandchildren. These grandchildren are the descendants of his nine children, including President John F. Kennedy, Senator Robert F. Kennedy, and other members of the Kennedy clan.\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6d770b4-80c5-49a6-a925-de3a800d19f2",
      "metadata": {
        "id": "a6d770b4-80c5-49a6-a925-de3a800d19f2"
      },
      "source": [
        "#### Old way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6dafd17-47a2-4169-992e-76ffb9702d89",
      "metadata": {
        "id": "c6dafd17-47a2-4169-992e-76ffb9702d89"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import HumanMessagePromptTemplate, ChatPromptTemplate\n",
        "\n",
        "# Create a ChatPromptTemplate that includes:\n",
        "# 1. A SystemMessage specifying the AI's role/knowledge domain\n",
        "# 2. A HumanMessagePromptTemplate to capture user queries\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are an Historian expert on the Kennedy family.\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Format the template by substituting actual user input\n",
        "messages = chat_template.format_messages(\n",
        "    user_input=\"Name the children and grandchildren of Joseph P. Kennedy?\"\n",
        ")\n",
        "\n",
        "# Invoke the chat model with the final set of messages\n",
        "response = chatModel.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ”¹ Whatâ€™s Happening?**\n",
        "1. **SystemMessage**:  \n",
        "   - `\"You are an Historian expert on the Kennedy family.\"` sets the context, telling the AI how to respond (as a historian).  \n",
        "\n",
        "2. **HumanMessagePromptTemplate.from_template(\"{user_input}\")**:  \n",
        "   - A placeholder `{user_input}` that can accept **any** user query or command.  \n",
        "   - Helps keep your code modular; you just **inject** different questions or instructions.\n",
        "\n",
        "3. **`chat_template.format_messages(...)`**:  \n",
        "   - Combines the **system** message and the **human** question into a **list** of role-based messages.  \n",
        "\n",
        "4. **`chatModel.invoke(messages)`**:  \n",
        "   - Sends this compiled conversation to the LLM, generating a **response** that presumably enumerates the children and grandchildren of Joseph P. Kennedy.\n",
        "\n",
        "## **ðŸ¤” Why This Matters**\n",
        "- **Less Clutter**: By splitting system vs. human messages, you avoid long, single-string prompts.  \n",
        "- **Easy Customization**: Switch the system role to a different **expert** or provide alternate instructions.  \n",
        "- **Scalability**: This approach forms the basis for more complex, multi-turn dialogues where each new user query is appended to the message list.\n",
        "\n",
        "## **ðŸ’¡ Pro Tips**\n",
        "- **Extend to More Roles**: If you need the AI to respond as an â€œAuthor,â€ â€œBiographer,â€ or â€œLawyer,â€ just tweak the system message.  \n",
        "- **Add Constraints**: Include style guidelines (e.g., â€œWrite in a formal tone.â€) in the system role.  \n",
        "- **Chaining**: If you want additional processing (like summarizing or code generation), you can chain the modelâ€™s response to subsequent stepsâ€”especially useful in multi-step workflows.\n",
        "\n",
        "> **ðŸ”Ž Remember**: System messages heavily influence the **tone** and **depth** of responses. A well-crafted system prompt can drastically enhance the quality and relevance of the LLMâ€™s output."
      ],
      "metadata": {
        "id": "DsAxf1pa4r99"
      },
      "id": "DsAxf1pa4r99"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4d5a5a-c3a3-48be-8b31-40b0881faa9a",
      "metadata": {
        "id": "2a4d5a5a-c3a3-48be-8b31-40b0881faa9a",
        "outputId": "5e93b10d-d7e1-4a28-80ad-768c0408bb28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joseph P. Kennedy and his wife Rose Fitzgerald Kennedy had nine children:\n",
            "\n",
            "1. Joseph P. Kennedy Jr.\n",
            "2. John F. Kennedy\n",
            "3. Rosemary Kennedy\n",
            "4. Kathleen Kennedy\n",
            "5. Eunice Kennedy\n",
            "6. Patricia Kennedy\n",
            "7. Robert F. Kennedy\n",
            "8. Jean Kennedy\n",
            "9. Edward M. Kennedy\n",
            "\n",
            "Their grandchildren include:\n",
            "\n",
            "- Caroline Kennedy (daughter of John F. Kennedy)\n",
            "- John F. Kennedy Jr. (son of John F. Kennedy)\n",
            "- Patrick J. Kennedy (son of Edward M. Kennedy)\n",
            "- Robert F. Kennedy Jr. (son of Robert F. Kennedy)\n",
            "- Maria Shriver (daughter of Eunice Kennedy)\n",
            "\n",
            "These are just a few examples of the grandchildren of Joseph P. Kennedy.\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b510dd0-33b7-4737-aef2-c52a1e8bbf41",
      "metadata": {
        "id": "0b510dd0-33b7-4737-aef2-c52a1e8bbf41"
      },
      "source": [
        "#### What is the full potential of ChatPromptTemplate?\n",
        "* Check the [corresponding page](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) in the LangChain API."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f8be09-99fc-491e-babb-0b8ad5bc74c2",
      "metadata": {
        "id": "d9f8be09-99fc-491e-babb-0b8ad5bc74c2"
      },
      "source": [
        "## Our first chain: an example of few-shot prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "301ae08a-e223-4239-8eca-8468ceb1e0d3",
      "metadata": {
        "id": "301ae08a-e223-4239-8eca-8468ceb1e0d3"
      },
      "outputs": [],
      "source": [
        "# Import the FewShotChatMessagePromptTemplate class\n",
        "from langchain_core.prompts import FewShotChatMessagePromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ”¹ FewShotChatMessagePromptTemplate**\n",
        "- **ðŸ§© Purpose**: Provides a mechanism to include **multiple example pairs** (inputâ†’output) in your chat prompt, helping the LLM understand **exactly** how you want it to respond.\n",
        "- **Examples**: Each dictionary in `examples` has `{\"input\": \"...\", \"output\": \"...\"}` pairs. The `example_prompt` is a `ChatPromptTemplate` that describes how these pairs should be presented (e.g., `(role=\"human\", content=\"{input}\")`, `(role=\"ai\", content=\"{output}\")`).\n",
        "\n",
        "## **âš™ï¸ How It Works**\n",
        "1. **`example_prompt`**: Defines the **structure** for each example (i.e., a human says something, the AI responds).  \n",
        "2. **`FewShotChatMessagePromptTemplate(...)`**:  \n",
        "   - Automatically **repeats** the `example_prompt` for every entry in `examples`.  \n",
        "   - Inserts them into your final prompt so the LLM can see sample interactions and mimic the style or format in its own response.\n",
        "\n",
        "## **ðŸ¤– Final Assembly**\n",
        "- **`final_prompt`**: Combines:  \n",
        "  1. **System** message (setting the AIâ€™s role as an English-Spanish translator).  \n",
        "  2. **Few-Shot Examples** (showing how short English phrases should be translated).  \n",
        "  3. **Human** message (the new phrase to translate).\n",
        "\n",
        "## **ðŸ’¡ Why Few-Shot Prompting?**\n",
        "- **Better Guidance**: By providing real examples, the model can more accurately infer your **preferred style** or **response format**.  \n",
        "- **Flexible**: Add or remove examples to **steer** the modelâ€™s output.  \n",
        "- **Time-Saving**: Reuse a single prompt template with different examples for new tasks or domains.\n",
        "\n",
        "> **ðŸš€ Pro Tip**: You can embed few-shot prompts in **chains**, enabling multi-step workflows that gather user input, provide examples, then feed everything into the model for a final, more **refined** answer."
      ],
      "metadata": {
        "id": "vqtAAo2-4-kp"
      },
      "id": "vqtAAo2-4-kp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b882accd-e3e7-4e8a-9d85-55ebaa945837",
      "metadata": {
        "id": "b882accd-e3e7-4e8a-9d85-55ebaa945837"
      },
      "outputs": [],
      "source": [
        "# Example input-output pairs demonstrating English-to-Spanish translations\n",
        "examples = [\n",
        "    {\"input\": \"hi!\", \"output\": \"Â¡hola!\"},\n",
        "    {\"input\": \"bye!\", \"output\": \"Â¡adiÃ³s!\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b973d828-0f47-4c43-98ab-e3db306d41e8",
      "metadata": {
        "id": "b973d828-0f47-4c43-98ab-e3db306d41e8",
        "outputId": "48975fa7-49be-4322-9bba-93f0e76a06d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Â¿QuiÃ©n fue JFK?', response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 52, 'total_tokens': 58}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c6f8ebf0-580f-43a6-8045-09511c0c84bd-0', usage_metadata={'input_tokens': 52, 'output_tokens': 6, 'total_tokens': 58})"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a ChatPromptTemplate for each example:\n",
        "# (human) \"hi!\" => (ai) \"Â¡hola!\"\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        # The \"human\" role contains a placeholder for the input text\n",
        "        (\"human\", \"{input}\"),\n",
        "        # The \"ai\" role contains a placeholder for the expected output translation\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a few-shot prompt using the example_prompt and the examples list\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "# Build a final chat prompt by combining:\n",
        "# 1) A system message stating the AI is an English-Spanish translator\n",
        "# 2) The few-shot examples from few_shot_prompt\n",
        "# 3) A human message with a placeholder for new input\n",
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an English-Spanish translator.\"),\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain the final prompt with the chat model, enabling direct invocation\n",
        "chain = final_prompt | chatModel\n",
        "\n",
        "# Invoke the chain with a new input to see how the model responds\n",
        "chain.invoke({\"input\": \"Who was JFK?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **âš™ï¸ Few-Shot Prompt Mechanism**\n",
        "- **ðŸ”§ `example_prompt`**: Each example is structured as a short conversation with **human** input â†’ **ai** output.  \n",
        "- **ðŸ—ƒ `FewShotChatMessagePromptTemplate`**: Automatically replicates `example_prompt` for every entry in `examples`, giving the model a **mini dataset** of how you expect translations to occur.\n",
        "\n",
        "### **ðŸ”— Final Prompt Assembly**\n",
        "- **`(\"system\", \"You are an English-Spanish translator.\")`**: Tells the model its role.  \n",
        "- **`few_shot_prompt`**: Inserts those example pairs so the AI sees how â€œhi!â€ maps to â€œÂ¡hola!â€, etc.  \n",
        "- **`(\"human\", \"{input}\")`**: Captures the userâ€™s new queryâ€”in this case, â€œWho was JFK?â€\n",
        "\n",
        "### **ðŸ¤– Chain Execution**\n",
        "- **`chain = final_prompt | chatModel`**: Combines your prompt with the model, creating a **callable chain**.  \n",
        "- **`chain.invoke(...)`**: Feeds the userâ€™s question (â€œWho was JFK?â€) into the chain, prompting the model to **translate** or respond in Spanish (as implied by the few-shot examples and system message).\n",
        "\n",
        "### **ðŸ’¡ Why This Matters**\n",
        "1. **Contextual Examples**: Few-shot examples demonstrate the **expected** style or format.  \n",
        "2. **Dynamic Interaction**: You can swap out examples for **different** translation tasks (e.g., formal vs. informal Spanish).  \n",
        "3. **Scalability**: The chain approach makes it easy to **expand** the pipeline with more steps (e.g., summarizing, analyzing) while reusing the same prompt logic.\n",
        "\n",
        "> **ðŸ’¼ Pro Tip**: If you want to handle more complex queries or additional examples, simply **add** more items to `examples` or refine your **system** message to shape the AIâ€™s translation style."
      ],
      "metadata": {
        "id": "79RtTLCw6es-"
      },
      "id": "79RtTLCw6es-"
    },
    {
      "cell_type": "markdown",
      "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444",
      "metadata": {
        "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444"
      },
      "source": [
        "## How to execute the code from Visual Studio Code\n",
        "* In Visual Studio Code, see the file 001-connect-llms.py\n",
        "* In terminal, make sure you are in the directory of the file and run:\n",
        "    * python 004-chains.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069",
      "metadata": {
        "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}