{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
      "metadata": {
        "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37"
      },
      "source": [
        "# Prompts and Prompt Templates\n",
        "* Introduce programming in your conversation with the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c759d27a-94f3-4141-945d-065f2095bffd",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "c759d27a-94f3-4141-945d-065f2095bffd"
      },
      "source": [
        "## Intro\n",
        "* Input: the prompt we send to the LLM.\n",
        "* Output: the response from the LLM.\n",
        "* We can switch LLMs and use several different LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8981f91b-686d-401c-a872-65487f93b46e",
      "metadata": {
        "id": "8981f91b-686d-401c-a872-65487f93b46e"
      },
      "source": [
        "## Table of contents\n",
        "* LLMs.\n",
        "* Prompts and Prompt Templates.\n",
        "* Types of prompts: Zero Shot and Few Shot(s) Prompt.\n",
        "* Serialization: Saving and Loading Prompts.\n",
        "* Parsing Outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8332b6e9-8164-4859-879c-f021a4dfd69d",
      "metadata": {
        "id": "8332b6e9-8164-4859-879c-f021a4dfd69d"
      },
      "source": [
        "## LangChain divides LLMs in two types\n",
        "1. LLM Model: text-completion model.\n",
        "2. Chat Model: converses with a sequence of messages and can have a particular role defined (system prompt). This type has become the most used in LangChain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de42a3ca-fc4d-4b91-b3bc-a7304ec4d5f8",
      "metadata": {
        "id": "de42a3ca-fc4d-4b91-b3bc-a7304ec4d5f8"
      },
      "source": [
        "## See the differences\n",
        "* Even when sometimes the LangChain documentation can be confusing about it, the fact is that text-completion models and Chat models are both LLMs.\n",
        "* But, as you can see in this [playground](https://platform.openai.com/playground/chat?models=gpt-4o), they have some significant differences. See that the chat models in LangChain have system messages, human messages (called \"user messages\" by OpenAI) and AI messages (called \"Assitant Messages\" by OpenAI).\n",
        "* Since the launch of chatGPT, the Chat Model is the most popular LLM type and is used in most LLM apps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a2f9501-fa9e-4830-95e2-537dff951cf1",
      "metadata": {
        "id": "5a2f9501-fa9e-4830-95e2-537dff951cf1"
      },
      "source": [
        "## List of LLMs that can work with LangChain\n",
        "* See the list [here](https://python.langchain.com/v0.1/docs/integrations/llms/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
      "metadata": {
        "id": "be46161e-45e9-46d7-8214-bcbea10aff2e"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871e0018-cba4-4959-881a-0a65093d202d",
      "metadata": {
        "id": "871e0018-cba4-4959-881a-0a65093d202d"
      },
      "source": [
        "#### After you download the code from the github repository in your computer\n",
        "In terminal:\n",
        "* cd project_name\n",
        "* pyenv local 3.11.4\n",
        "* poetry install\n",
        "* poetry shell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80616acf-ae85-4226-ba19-dbbbb9d4796f",
      "metadata": {
        "id": "80616acf-ae85-4226-ba19-dbbbb9d4796f"
      },
      "source": [
        "#### To open the notebook with Jupyter Notebooks\n",
        "In terminal:\n",
        "* jupyter lab\n",
        "\n",
        "Go to the folder of notebooks and open the right notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a00a6725-81b3-4ea3-b39a-eb8f2ded91a2",
      "metadata": {
        "id": "a00a6725-81b3-4ea3-b39a-eb8f2ded91a2"
      },
      "source": [
        "#### To see the code in Virtual Studio Code or your editor of choice.\n",
        "* open Virtual Studio Code or your editor of choice.\n",
        "* open the project-folder\n",
        "* open the 003-prompt-templates.py file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2",
      "metadata": {
        "id": "af743328-1bc8-4b01-85fb-fcb21c6499c2"
      },
      "source": [
        "## Create your .env file\n",
        "* In the github repo we have included a file named .env.example\n",
        "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
        "* OPENAI_API_KEY=your_openai_api_key\n",
        "* LANGCHAIN_TRACING_V2=true\n",
        "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
        "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
        "* LANGCHAIN_PROJECT=your_project_name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "863dd299-0780-49ad-a1b7-b76e249350da",
      "metadata": {
        "id": "863dd299-0780-49ad-a1b7-b76e249350da"
      },
      "source": [
        "We will call our LangSmith project **003-prompt-templates**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2746c97-9fa5-481c-8333-21de1504a087",
      "metadata": {
        "id": "a2746c97-9fa5-481c-8333-21de1504a087"
      },
      "source": [
        "## Track operations\n",
        "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
        "* [smith.langchain.com](https://smith.langchain.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291",
      "metadata": {
        "id": "de4ba351-2cfb-4b93-9c79-3c1100e2e291"
      },
      "source": [
        "## Connect with the .env file located in the same directory of this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a",
      "metadata": {
        "id": "36eaf7e9-acf2-4729-b54c-a8fb6ad2ae1a"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a10870-432e-4818-aa5e-6be24c579d39",
      "metadata": {
        "id": "e9a10870-432e-4818-aa5e-6be24c579d39"
      },
      "outputs": [],
      "source": [
        "#pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing the `python-dotenv` Package\n",
        "\n",
        "## What is `python-dotenv`?\n",
        "`python-dotenv` is a Python package that loads environment variables from a `.env` file into the system's environment. It is commonly used in applications where sensitive information (e.g., API keys, database credentials) needs to be stored securely outside the source code.\n",
        "\n",
        "## Why Use `python-dotenv`?\n",
        "- **Security**: Keeps credentials out of source code.\n",
        "- **Environment Management**: Allows easy switching between development, staging, and production settings.\n",
        "- **Project Organization**: Centralizes configuration values for better maintainability.\n",
        "\n",
        "## Installation Command\n",
        "To install `python-dotenv`, run:\n",
        "\n",
        "```python\n",
        "!pip install python-dotenv\n",
        "```\n",
        "\n",
        "## Explanation of the Installation Command:\n",
        "- `!` â†’ Used in Jupyter Notebooks to run shell commands.\n",
        "- `pip` â†’ Pythonâ€™s package manager for installing libraries.\n",
        "- `install` â†’ Specifies that we want to install a package.\n",
        "- `python-dotenv` â†’ The package being installed.\n",
        "\n",
        "Once installed, we can use `python-dotenv` to manage environment variables in our Python scripts.\n"
      ],
      "metadata": {
        "id": "gjMw1QwVbS7H"
      },
      "id": "gjMw1QwVbS7H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
      "metadata": {
        "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ” Detailed Explanation of the Code Block**\n",
        "\n",
        "## **ðŸŽ¯ Objective**\n",
        "This code snippet is used to load environment variables from a `.env` file and retrieve the `OPENAI_API_KEY` from the system's environment variables.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Breaking Down the Code**\n",
        "### **1ï¸âƒ£ Importing Required Modules**\n",
        "```python\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "```\n",
        "- **`os` module**: Provides functions to interact with the operating system, such as reading environment variables.\n",
        "- **`dotenv` module**: Helps in loading environment variables from a `.env` file into the system's environment.\n",
        "\n",
        "---\n",
        "\n",
        "### **2ï¸âƒ£ Finding and Loading the Environment File**\n",
        "```python\n",
        "_ = load_dotenv(find_dotenv())\n",
        "```\n",
        "- **`find_dotenv()`**: Searches for a `.env` file in the current directory or parent directories.\n",
        "- **`load_dotenv()`**: Loads the variables defined in the `.env` file into the system environment.\n",
        "- **Why assign to `_`?** The underscore `_` is used as a throwaway variable, meaning we donâ€™t need to store any return value.\n",
        "\n",
        "---\n",
        "\n",
        "### **3ï¸âƒ£ Retrieving the OpenAI API Key**\n",
        "```python\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "```\n",
        "- **`os.environ`**: A dictionary that holds the system's environment variables.\n",
        "- **Accessing `OPENAI_API_KEY`**: Retrieves the OpenAI API key stored in the environment.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Key Takeaways**\n",
        "âœ… This approach ensures that sensitive credentials are not hardcoded in the script.  \n",
        "âœ… The `.env` file should contain a line like `OPENAI_API_KEY=your_secret_key` for this to work.  \n",
        "âœ… Using environment variables helps improve security and maintainability.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "FPPwvtOAhibQ"
      },
      "id": "FPPwvtOAhibQ"
    },
    {
      "cell_type": "markdown",
      "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
      "metadata": {
        "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80"
      },
      "source": [
        "#### Install LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa888f7-3718-4829-8645-30acb43db51f",
      "metadata": {
        "id": "6fa888f7-3718-4829-8645-30acb43db51f"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
      "metadata": {
        "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“Œ Step 2: Installing the LangChain Library**\n",
        "\n",
        "## **ðŸŽ¯ Objective**\n",
        "- **Install the `langchain` library** to leverage its tools for building LLM-powered applications.\n",
        "- **Ensure** all required dependencies are correctly installed to avoid errors in subsequent steps.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ”¹ Code Breakdown**\n",
        "    #!pip install langchain\n",
        "\n",
        "### **1ï¸âƒ£ What Does `#!pip install langchain` Do?**\n",
        "- **Installs the latest version** of LangChain from the Python Package Index (PyPI).\n",
        "- Itâ€™s a **shell command** that can be run directly within a Jupyter notebook cell (hence `#!`), or in a standard terminal by removing the `#!`.\n",
        "\n",
        "### **2ï¸âƒ£ Why Install LangChain?**\n",
        "- **ðŸ§© Modular Components**: LangChain provides ready-made modules for prompts, chains, agents, and memory, simplifying LLM app development.\n",
        "- **ðŸ¤– LLM Flexibility**: It supports multiple LLMs, including both text-completion and chat-based models.\n",
        "- **âš¡ Speed & Efficiency**: Common operations like prompt engineering, conversation management, and tool usage are streamlined.\n",
        "\n",
        "### **3ï¸âƒ£ Key Points & Dependencies**\n",
        "- This line will **automatically resolve any sub-dependencies**.  \n",
        "- **Version Pinning** (e.g., `langchain==0.0.x`) can be used if you want a specific release, but leaving it unpinned installs the latest available version.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Key Takeaways**\n",
        "1. **LangChain** is a versatile framework that **abstracts away** much of the complexity of working with LLMs.\n",
        "2. A simple **`pip install langchain`** is all thatâ€™s needed to get started in most environments.\n",
        "3. Once installed, you can **import** LangChain modules and begin building prompt workflows, tools, and data pipelines for LLM-powered solutions.\n",
        "\n",
        "> *LangChain is like a Swiss Army knife for LLM tasksâ€”just a single install and youâ€™ve got an arsenal of handy tools at your disposal!*\n"
      ],
      "metadata": {
        "id": "wcqfmM393Yal"
      },
      "id": "wcqfmM393Yal"
    },
    {
      "cell_type": "markdown",
      "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
      "metadata": {
        "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941"
      },
      "source": [
        "## Connect with an LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d5d5b71-b26a-4cd5-9765-019077a67141",
      "metadata": {
        "id": "4d5d5b71-b26a-4cd5-9765-019077a67141"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
      "metadata": {
        "id": "148df8e0-361d-4ddd-8709-af48fa1648d1"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“Œ Step 3: Installing the `langchain-openai` Library**\n",
        "\n",
        "## **ðŸŽ¯ Objective**\n",
        "- **Install the `langchain-openai` library** to seamlessly integrate OpenAI models with LangChain.\n",
        "- **Enable** advanced functionality specific to OpenAIâ€™s LLMs (e.g., GPT-3.5, GPT-4).\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ”¹ Code Breakdown**\n",
        "    #!pip install langchain-openai\n",
        "\n",
        "### **1ï¸âƒ£ What Does `#!pip install langchain-openai` Do?**\n",
        "- **Installs** an extension package for LangChain focused on OpenAI services.\n",
        "- Adds **convenience classes and functions** that simplify usage of OpenAIâ€™s APIs (e.g., ChatCompletion, text embeddings, etc.).\n",
        "- **`#!`** indicates that this command can be run as a cell in your notebook. If youâ€™re using a terminal, simply drop the `#!`.\n",
        "\n",
        "---\n",
        "\n",
        "### **2ï¸âƒ£ Why `langchain-openai` Specifically?**\n",
        "- **OpenAI Integrations**: Offers robust wrappers for GPT-3.5, GPT-4, and future OpenAI models.\n",
        "- **End-to-End Development**: Provides higher-level abstractions like conversation chains, memory, and chat-based flowsâ€”all tailored to OpenAIâ€™s endpoints.\n",
        "- **Streamlined Prompt Engineering**: Tools like message formatting, token usage estimation, and more come pre-built.\n",
        "\n",
        "---\n",
        "\n",
        "### **3ï¸âƒ£ Dependency & Compatibility Checks**\n",
        "- **Ensure** you already have `langchain` installed. This package **enhances** that base functionality.\n",
        "- **Python Version**: Usually requires Python 3.7+ for optimal performance.  \n",
        "- **OpenAI Key**: Youâ€™ll still need the environment variable `OPENAI_API_KEY` or a mechanism to provide your API key for it to work properly.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Key Takeaways**\n",
        "1. `langchain-openai` extends the base LangChain library with **OpenAI-specific** utilities and integration points.  \n",
        "2. Running `pip install langchain-openai` ensures you have **all** the specialized tools for creating powerful LLM applications with OpenAI models.  \n",
        "3. Once installed, you can easily **import** these additional classes and functionalities alongside the standard LangChain modules.\n",
        "\n",
        "> *Think of `langchain-openai` as the VIP backstage pass that lets LangChain seamlessly interact with the best OpenAI has to offer!*\n"
      ],
      "metadata": {
        "id": "ukp8G4eo3fGJ"
      },
      "id": "ukp8G4eo3fGJ"
    },
    {
      "cell_type": "markdown",
      "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
      "metadata": {
        "id": "a1998155-91de-4cbc-bc88-8d77beefb51b"
      },
      "source": [
        "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27",
      "metadata": {
        "id": "fbf1d17b-6d15-423b-b554-26d6d977ca27"
      },
      "source": [
        "## LLM Model\n",
        "* The trend before the launch of chatGPT-4.\n",
        "* See LangChain documentation about LLM Models [here](https://python.langchain.com/v0.1/docs/modules/model_io/llms/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92628f2-62e8-436c-92d4-e849de7744ad",
      "metadata": {
        "id": "e92628f2-62e8-436c-92d4-e849de7744ad"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llmModel = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“Œ Step 4: Initializing an OpenAI LLM with `langchain-openai`**\n",
        "\n",
        "## **ðŸŽ¯ Objective**\n",
        "- **Create** an instance of the `OpenAI` class from the `langchain_openai` library.\n",
        "- **Establish** a basic LLM model object (`llmModel`) that can be used for text completion, chat, or other LLM-driven tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ”¹ Code Breakdown**\n",
        "    from langchain_openai import OpenAI\n",
        "    \n",
        "    llmModel = OpenAI()\n",
        "\n",
        "### **1ï¸âƒ£ `from langchain_openai import OpenAI`**\n",
        "- **Imports** the `OpenAI` class, a specialized integration for OpenAIâ€™s GPT models.\n",
        "- This class typically includes methods and parameters to interact with endpoints like **text-completion** or **chat**.\n",
        "\n",
        "### **2ï¸âƒ£ `llmModel = OpenAI()`**\n",
        "- **Instantiates** the `OpenAI` object using default parameters.\n",
        "- Under the hood, it relies on your **OpenAI API key** (which you set via environment variables).\n",
        "- You can optionally pass arguments like `temperature`, `model_name`, or `max_tokens` to customize the LLMâ€™s behavior.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Usage & Configuration**\n",
        "1. **Temperature Control**: Adjusts the randomness of the model output.  \n",
        "   - **Lower** values (0.0â€“0.3) â†’ More deterministic, consistent answers.  \n",
        "   - **Higher** values (0.7â€“1.0) â†’ More creative, varied responses.  \n",
        "\n",
        "2. **Model Selection**:  \n",
        "   - You can set `model_name=\"gpt-3.5-turbo\"` (or another model) if your account has access.  \n",
        "   - The default might be an older or general model version.  \n",
        "\n",
        "3. **API Key Access**:  \n",
        "   - Make sure your environment variable `OPENAI_API_KEY` is set.  \n",
        "   - If not, youâ€™ll need to supply the key explicitly or set it in your `.env` file.  \n",
        "\n",
        "4. **Chat vs. Completion**:  \n",
        "   - This class can handle both conversation-based calls (chat) or single-shot completions.  \n",
        "   - In your code, you might use `llmModel()` or designated methods for generating responses.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Key Takeaways**\n",
        "1. **`OpenAI`** is your direct interface for harnessing GPT models.  \n",
        "2. **Configuration** is handled via parameters (e.g., `temperature`, `model_name`) or environment variables for **API keys**.  \n",
        "3. Once created, `llmModel` can be used in various LangChain constructs (chains, agents, prompts) to enable powerful LLM-driven capabilities.\n",
        "\n",
        "> *Think of `llmModel` as your personal AI collaboratorâ€”give it instructions, and it crafts text with the style or creativity you specify!*\n"
      ],
      "metadata": {
        "id": "QkvDoemQ3sFN"
      },
      "id": "QkvDoemQ3sFN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7",
      "metadata": {
        "id": "b14d27c3-0b1b-4b11-a883-8da2734f21a7"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“Œ Step 5: Using the `ChatOpenAI` Class for Conversational Models**\n",
        "\n",
        "## **ðŸŽ¯ Objective**\n",
        "- **Instantiate** a chat-capable LLM model (`ChatOpenAI`) configured with a specific model version.\n",
        "- **Showcase** how to tailor the underlying OpenAI chat API by specifying model parameters (e.g., `model=\"gpt-3.5-turbo-0125\"`).\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ”¹ Code Breakdown**\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    \n",
        "    chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "### **1ï¸âƒ£ `from langchain_openai import ChatOpenAI`**\n",
        "- **Imports** the `ChatOpenAI` class, enabling chat-based interactions with OpenAIâ€™s conversational endpoints.\n",
        "- This differs from `OpenAI()` in that itâ€™s specifically designed for **multi-turn conversations** and role-based prompts.\n",
        "\n",
        "### **2ï¸âƒ£ `chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")`**\n",
        "- **Creates** a chat model instance tied to the `\"gpt-3.5-turbo-0125\"` version of GPT.\n",
        "- **model** parameter ensures youâ€™re using a specific release of the GPT-3.5 familyâ€”useful if you need consistent behavior tied to that version.\n",
        "- As with other `langchain_openai` classes, it looks for the `OPENAI_API_KEY` in the environment.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Configuration Details**\n",
        "1. **Chat-Centric Interface**  \n",
        "   - Allows **system**, **user**, and **assistant** messages to be passed explicitly.  \n",
        "   - Streamlines multi-turn dialogs, persona-driven interactions, and memory usage.\n",
        "\n",
        "2. **Model Version**  \n",
        "   - `gpt-3.5-turbo-0125` is a snapshot of GPT-3.5-turbo with potential minor differences.  \n",
        "   - Specifying the model ensures consistent output and avoids unexpected changes when OpenAI updates their base models.\n",
        "\n",
        "3. **Additional Parameters**  \n",
        "   - **`temperature`**: Controls randomness (like with `OpenAI()`).  \n",
        "   - **`top_p`, `max_tokens`, `streaming`**: Additional controls over generation style or output size.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Key Takeaways**\n",
        "1. **`ChatOpenAI`** provides a more **conversation-focused** approach compared to the generic `OpenAI` class.  \n",
        "2. **Specifying `model=\"gpt-3.5-turbo-0125\"`** ensures youâ€™re locked to a particular version of GPT, preserving consistency over time.  \n",
        "3. **Parameters** like `temperature` and `max_tokens` can be used to customize the chat modelâ€™s response style and length.  \n",
        "4. This class still uses your **OpenAI API Key**; always confirm that environment variables are set or passed properly.\n",
        "\n",
        "> *Think of `ChatOpenAI` as the talkative sibling of `OpenAI`, perfectly suited for dynamic, multi-message back-and-forth conversations!*\n"
      ],
      "metadata": {
        "id": "szvoZ2ud3zD1"
      },
      "id": "szvoZ2ud3zD1"
    },
    {
      "cell_type": "markdown",
      "id": "c2f91601-8594-41d3-9316-d51791fc54e8",
      "metadata": {
        "id": "c2f91601-8594-41d3-9316-d51791fc54e8"
      },
      "source": [
        "## Prompts and Prompt Templates\n",
        "A **prompt** is the input we provide to one language model. This input will guide the way the language model will respond.\n",
        "There are many types of prompts:\n",
        "* Plain instructions.\n",
        "* Instructions with a few examples (few-shot examples).\n",
        "* Specific context and questions appropiate for a given task.\n",
        "* Etc.\n",
        "* See the LangChain documentation about prompts [here](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/).\n",
        "\n",
        "**Prompt templates** are pre-defined prompt recipes that usually need some extra pieces to be complete. These extra pieces are variables that the user will provide.\n",
        "* Prompt templates: when we want to use sophisticated prompts with variables and other elements. A prompt template may include:\n",
        "    * instructions,\n",
        "    * few-shot examples,\n",
        "    * and specific context and questions appropriate for a given task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5383c7e-7e62-46d0-8bef-17ff45a88495",
      "metadata": {
        "id": "a5383c7e-7e62-46d0-8bef-17ff45a88495",
        "outputId": "6a095044-5cb4-4b51-b7be-723bd8050f9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n\\nOne curious story about the Kennedy family involves a supposed curse that has followed them for generations. It is said that the curse originated with JFK's grandfather, John F. Fitzgerald, who was known for his womanizing ways and was rumored to have made a deal with the devil for political success.\\n\\nThe curse continued with JFK's father, Joseph P. Kennedy, who was involved in various scandals and controversies, including his alleged involvement in bootlegging during Prohibition. He also famously predicted that three of his sons would die young, which unfortunately came true with the assassinations of JFK, Robert F. Kennedy, and John F. Kennedy Jr.\\n\\nThe curse seemed to extend to JFK's children as well. His daughter, Caroline Kennedy, was involved in a skiing accident that left her with a severe concussion. His son, John F. Kennedy Jr., died in a plane crash along with his wife and sister-in-law. And his other son, Patrick Kennedy, was born prematurely and died just two days later.\\n\\nEven JFK's grandchildren have not been spared from the curse. One of his grandsons, John F. Kennedy III, was arrested for disorderly conduct and resisting arrest, and another grandson, William Kennedy Smith, was accused of rape.\\n\\nWhile some may dismiss the curse as\""
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} story about {topic}.\"\n",
        ")\n",
        "\n",
        "llmModelPrompt = prompt_template.format(\n",
        "    adjective=\"curious\",\n",
        "    topic=\"the Kennedy family\"\n",
        ")\n",
        "\n",
        "llmModel.invoke(llmModelPrompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“Œ Step 6: Using `PromptTemplate` to Generate Custom Prompt Strings**\n",
        "\n",
        "## **ðŸŽ¯ Objective**\n",
        "- **Leverage** the `PromptTemplate` class to build parameterized prompts dynamically.\n",
        "- **Invoke** the LLM model (`llmModel`) with a **formatted** prompt, demonstrating how to insert custom placeholders.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ”¹ Code Snippet**\n",
        "\n",
        "    from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "    prompt_template = PromptTemplate.from_template(\n",
        "        \"Tell me a {adjective} story about {topic}.\"\n",
        "    )\n",
        "\n",
        "    llmModelPrompt = prompt_template.format(\n",
        "        adjective=\"curious\",\n",
        "        topic=\"the Kennedy family\"\n",
        "    )\n",
        "\n",
        "    llmModel.invoke(llmModelPrompt)\n",
        "\n",
        "---\n",
        "\n",
        "## **1ï¸âƒ£ What Does This Code Do?**\n",
        "\n",
        "1. **Import `PromptTemplate`**:  \n",
        "   - `from langchain_core.prompts import PromptTemplate` brings in LangChainâ€™s mechanism for **parameterized prompts**.\n",
        "\n",
        "2. **Define a Prompt Template**:  \n",
        "   - `PromptTemplate.from_template(\"Tell me a {adjective} story about {topic}.\")` creates a reusable **template** with two placeholders: `{adjective}` and `{topic}`.\n",
        "\n",
        "3. **Format the Prompt**:  \n",
        "   - `llmModelPrompt = prompt_template.format(adjective=\"curious\", topic=\"the Kennedy family\")` fills in those placeholders, resulting in:  \n",
        "     **\"Tell me a curious story about the Kennedy family.\"**\n",
        "\n",
        "4. **Invoke the Model**:  \n",
        "   - `llmModel.invoke(llmModelPrompt)` sends the final prompt string to your LLM (`llmModel`), receiving a **text-based response**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2ï¸âƒ£ Why Use `PromptTemplate`?**\n",
        "\n",
        "- **Modular & Reusable**: Change the `adjective` or `topic` without rewriting your entire prompt.\n",
        "- **Consistency**: Reduces errors by keeping prompt structure in one central place.\n",
        "- **Scalability**: In larger applications, you can manage multiple templates (for summarizing, questioning, etc.) uniformly.\n",
        "\n",
        "---\n",
        "\n",
        "## **3ï¸âƒ£ Example Output**\n",
        "\n",
        "_The example output you shared shows a narrative about a supposed â€œKennedy curse.â€_  \n",
        "- The **LLM** elaborated on historical rumors tied to the family, aligning with the **â€œcurious storyâ€** instruction.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Key Takeaways**\n",
        "\n",
        "1. **`PromptTemplate`** separates **prompt logic** from code, letting you easily adapt or expand prompts.  \n",
        "2. **Parameterizing placeholders** (`{adjective}`, `{topic}`) is straightforward and reduces repetitive text.  \n",
        "3. **Model Invocation** (`.invoke(...)`) then processes the fully-formed prompt, returning context-specific outputs.\n",
        "\n",
        "> *Imagine `PromptTemplate` as a stencil: you supply the unique parts (adjective & topic), and it paints the final story with ease!*\n"
      ],
      "metadata": {
        "id": "HoSgixyj5GUP"
      },
      "id": "HoSgixyj5GUP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb7dbcd-524c-4acc-9b65-0b209a170ad7",
      "metadata": {
        "id": "beb7dbcd-524c-4acc-9b65-0b209a170ad7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
        "        (\"human\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
        "        (\"ai\", \"Sure!\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_template.format_messages(\n",
        "    profession=\"Historian\",\n",
        "    topic=\"The Kennedy family\",\n",
        "    user_input=\"How many grandchildren had Joseph P. Kennedy?\"\n",
        ")\n",
        "\n",
        "response = chatModel.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“Œ Step 7: Using `ChatPromptTemplate` for Multi-Message Interaction**\n",
        "\n",
        "## **ðŸŽ¯ Objective**\n",
        "- **Create a conversation flow** with distinct message roles (system, human, AI).\n",
        "- **Format** those messages dynamically using placeholders (e.g., `{profession}`, `{topic}`, `{user_input}`).\n",
        "- **Invoke** a chat-based LLM (`chatModel`) to respond based on the structured conversation.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ”¹ Code Snippet**\n",
        "\n",
        "    from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "    chat_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an {profession} expert on {topic}.\"),\n",
        "            (\"human\", \"Hello, Mr. {profession}, can you please answer a question?\"),\n",
        "            (\"ai\", \"Sure!\"),\n",
        "            (\"human\", \"{user_input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    messages = chat_template.format_messages(\n",
        "        profession=\"Historian\",\n",
        "        topic=\"The Kennedy family\",\n",
        "        user_input=\"How many grandchildren had Joseph P. Kennedy?\"\n",
        "    )\n",
        "\n",
        "    response = chatModel.invoke(messages)\n",
        "\n",
        "---\n",
        "\n",
        "## **1ï¸âƒ£ What Does `ChatPromptTemplate` Do?**\n",
        "1. **Role-Based Construction**  \n",
        "   - You define messages for **system**, **human**, and **ai** roles.  \n",
        "   - Each tuple in the list represents `(role, template_string)`.\n",
        "\n",
        "2. **`from_messages` Method**  \n",
        "   - Accepts a list of message templates.  \n",
        "   - Integrates placeholders (e.g., `{profession}`, `{topic}`, `{user_input}`) which will be filled at runtime.\n",
        "\n",
        "3. **`format_messages(...)`**  \n",
        "   - Substitutes placeholder values (like `\"Historian\"`, `\"The Kennedy family\"`) into each message.  \n",
        "   - Produces a list of **fully-rendered** messages ready to send to the LLM.\n",
        "\n",
        "4. **`chatModel.invoke(messages)`**  \n",
        "   - Calls the **chat-based** LLM, passing the entire conversation context (system instructions, prior AI and human messages).  \n",
        "   - Returns a **single response** from the LLM that continues the dialogue.\n",
        "\n",
        "---\n",
        "\n",
        "## **2ï¸âƒ£ Roles & Placeholders Explained**\n",
        "\n",
        "- **System**: Provides overarching instructions or context, e.g., â€œYou are an expert on the Kennedy family.â€  \n",
        "- **Human**: Simulates real user input, e.g., â€œHello, Mr. Historian...â€ and â€œHow many grandchildren...?â€  \n",
        "- **AI**: Acts as the previous AI response in the conversation. You can inject default or example responses to guide style or tone.  \n",
        "\n",
        "**Why it matters**:  \n",
        "- This structure closely mirrors how **OpenAI Chat APIs** expect role-based messages, allowing for multi-turn dialogues and improved context retention.\n",
        "\n",
        "---\n",
        "\n",
        "## **3ï¸âƒ£ Example Output**\n",
        "\n",
        "- After invocation, `response` might address the question about **Joseph P. Kennedyâ€™s grandchildren** with a concise or detailed historical explanation.\n",
        "- The **LLM** can factor in the systemâ€™s role instructions (â€œYou are a Historian...â€) to generate more authoritative or context-rich answers.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Key Takeaways**\n",
        "1. **ChatPromptTemplate** enables a **role-specific** prompt design that replicates real conversation flows.  \n",
        "2. **Placeholders** let you swap out roles, topics, and user queries without altering the underlying message structure.  \n",
        "3. This approach ensures the LLM sees a clear **conversation history**, guiding it to respond appropriately.\n",
        "\n",
        "> *Think of `ChatPromptTemplate` as the scriptwriter for a stage playâ€”each role has defined lines, and placeholders let you quickly rewrite scenes for new characters and plots!*\n"
      ],
      "metadata": {
        "id": "3wwrluET5ho4"
      },
      "id": "3wwrluET5ho4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7585be48-9a19-43ab-a275-8ef6a04246a7",
      "metadata": {
        "id": "7585be48-9a19-43ab-a275-8ef6a04246a7",
        "outputId": "22502dcf-294a-46f8-aef2-331f30dd9766"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Joseph P. Kennedy, Sr. and his wife Rose Fitzgerald Kennedy had a total of nine grandchildren. Their children were John F. Kennedy, Robert F. Kennedy, Ted Kennedy, Eunice Kennedy Shriver, Patricia Kennedy Lawford, Jean Kennedy Smith, and Rosemary Kennedy.', response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 55, 'total_tokens': 112}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-970c06fd-afd3-4b21-8e9e-f51e97ab6511-0', usage_metadata={'input_tokens': 55, 'output_tokens': 57, 'total_tokens': 112})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1373ca0b-342f-4ac6-a697-5e2ad30bfa8c",
      "metadata": {
        "id": "1373ca0b-342f-4ac6-a697-5e2ad30bfa8c",
        "outputId": "3b81ae04-6972-4f79-e1ac-8721e5989fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Joseph P. Kennedy, Sr. and his wife Rose Fitzgerald Kennedy had a total of nine grandchildren. Their children were John F. Kennedy, Robert F. Kennedy, Ted Kennedy, Eunice Kennedy Shriver, Patricia Kennedy Lawford, Jean Kennedy Smith, and Rosemary Kennedy.' response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 55, 'total_tokens': 112}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-970c06fd-afd3-4b21-8e9e-f51e97ab6511-0' usage_metadata={'input_tokens': 55, 'output_tokens': 57, 'total_tokens': 112}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f6a5e25-e177-4d7a-ba29-6834a6a8152b",
      "metadata": {
        "id": "6f6a5e25-e177-4d7a-ba29-6834a6a8152b",
        "outputId": "6892f5eb-6342-4ee2-e839-e27214e9d668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joseph P. Kennedy, Sr. and his wife Rose Fitzgerald Kennedy had a total of nine grandchildren. Their children were John F. Kennedy, Robert F. Kennedy, Ted Kennedy, Eunice Kennedy Shriver, Patricia Kennedy Lawford, Jean Kennedy Smith, and Rosemary Kennedy.\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6d770b4-80c5-49a6-a925-de3a800d19f2",
      "metadata": {
        "id": "a6d770b4-80c5-49a6-a925-de3a800d19f2"
      },
      "source": [
        "#### Old way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6dafd17-47a2-4169-992e-76ffb9702d89",
      "metadata": {
        "id": "c6dafd17-47a2-4169-992e-76ffb9702d89"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import HumanMessagePromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are an Historian expert on the Kennedy family.\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_template.format_messages(\n",
        "    user_input=\"Name the children and grandchildren of Joseph P. Kennedy?\"\n",
        ")\n",
        "\n",
        "response = chatModel.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“Œ Step 8: Mixing System Messages & `HumanMessagePromptTemplate`**\n",
        "\n",
        "## **ðŸŽ¯ Objective**\n",
        "- **Define** a system message explicitly using `SystemMessage` to provide role-based context.\n",
        "- **Incorporate** a human message prompt template that dynamically injects user input.\n",
        "- **Generate** a conversation flow that the chat model (`chatModel`) can respond to.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ”¹ Code Snippet**\n",
        "\n",
        "    from langchain_core.messages import SystemMessage\n",
        "    from langchain_core.prompts import HumanMessagePromptTemplate\n",
        "\n",
        "    chat_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            SystemMessage(\n",
        "                content=(\n",
        "                    \"You are an Historian expert on the Kennedy family.\"\n",
        "                )\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    messages = chat_template.format_messages(\n",
        "        user_input=\"Name the children and grandchildren of Joseph P. Kennedy?\"\n",
        "    )\n",
        "\n",
        "    response = chatModel.invoke(messages)\n",
        "\n",
        "---\n",
        "\n",
        "## **1ï¸âƒ£ Whatâ€™s Happening Here?**\n",
        "\n",
        "### **A. SystemMessage**\n",
        "- **SystemMessage** is used to **define the AIâ€™s role** and context:\n",
        "  - Content: `\"You are an Historian expert on the Kennedy family.\"`\n",
        "  - **Purpose**: Sets the AIâ€™s persona or knowledge domain.\n",
        "\n",
        "### **B. `HumanMessagePromptTemplate.from_template`**\n",
        "- **Placeholder**: `\"{user_input}\"` allows us to inject a userâ€™s question dynamically.\n",
        "- **Why?**: This design keeps your prompt modular: you can easily switch or expand the template in future queries.\n",
        "\n",
        "### **C. `chat_template.format_messages(...)`**\n",
        "- **Substitutes** `{user_input}` with `\"Name the children and grandchildren of Joseph P. Kennedy?\"`.\n",
        "- **Returns** a fully formatted list of messages:  \n",
        "  1. **System** message with historian context.  \n",
        "  2. **Human** message with the userâ€™s actual question.\n",
        "\n",
        "### **D. `chatModel.invoke(messages)`**\n",
        "- **Sends** the conversation to the chat model.\n",
        "- **Receives** the AIâ€™s response, presumably enumerating **Joseph P. Kennedyâ€™s** offspring and grandchildren if known.\n",
        "\n",
        "---\n",
        "\n",
        "## **2ï¸âƒ£ Why Use `SystemMessage` & `HumanMessagePromptTemplate`?**\n",
        "\n",
        "1. **Explicit Role Control**  \n",
        "   - `SystemMessage` clarifies the AIâ€™s function or expertise, enhancing consistency and accuracy in responses.\n",
        "2. **Flexibility**  \n",
        "   - `HumanMessagePromptTemplate` keeps user-driven content distinct from system-level instructions.\n",
        "3. **Scalability**  \n",
        "   - In more complex apps, you might add multiple system or AI messages (e.g., clarifications, instructions, or example Q&As) to guide the conversation thoroughly.\n",
        "\n",
        "---\n",
        "\n",
        "## **3ï¸âƒ£ Example Use Case**\n",
        "- If youâ€™re building a **historical Q&A bot** about the Kennedy family, you can keep the **system** role pinned as an â€œExpert Historian,â€ while the **human** side dynamically updates with new questions.  \n",
        "- Each query is then shaped by the historian persona, maintaining a consistent style and expertise level.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Key Takeaways**\n",
        "1. **System vs. Human Role**: Cleanly separates overarching instructions from user queries.  \n",
        "2. **`format_messages`** provides a robust way to pass unique inputs without rewriting prompt logic.  \n",
        "3. **Conversation Flow**: The model sees a conversation context that makes responses more coherent and specialized.\n",
        "\n",
        "> *Think of `SystemMessage` as the stage director who sets the scene, while `HumanMessagePromptTemplate` delivers the audienceâ€™s linesâ€”together, they bring your AI conversation to life!*\n"
      ],
      "metadata": {
        "id": "lkRCT2mv5qEb"
      },
      "id": "lkRCT2mv5qEb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4d5a5a-c3a3-48be-8b31-40b0881faa9a",
      "metadata": {
        "id": "2a4d5a5a-c3a3-48be-8b31-40b0881faa9a",
        "outputId": "040f1e9a-559c-4d2e-87c5-65b8696760e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joseph P. Kennedy and his wife Rose Fitzgerald Kennedy had nine children:\n",
            "\n",
            "1. Joseph P. Kennedy Jr.\n",
            "2. John F. Kennedy\n",
            "3. Rosemary Kennedy\n",
            "4. Kathleen Kennedy\n",
            "5. Eunice Kennedy\n",
            "6. Patricia Kennedy\n",
            "7. Robert F. Kennedy\n",
            "8. Jean Kennedy\n",
            "9. Edward M. Kennedy\n",
            "\n",
            "The grandchildren of Joseph P. Kennedy include Caroline Kennedy (daughter of John F. Kennedy), Maria Shriver (daughter of Eunice Kennedy), and Robert F. Kennedy Jr. (son of Robert F. Kennedy), among others.\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b510dd0-33b7-4737-aef2-c52a1e8bbf41",
      "metadata": {
        "id": "0b510dd0-33b7-4737-aef2-c52a1e8bbf41"
      },
      "source": [
        "#### What is the full potential of ChatPromptTemplate?\n",
        "* Check the [corresponding page](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) in the LangChain API."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502b9539",
      "metadata": {
        "id": "502b9539"
      },
      "source": [
        "## Basic prompting strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a985bbc",
      "metadata": {
        "id": "2a985bbc"
      },
      "source": [
        "* Zero Shot Prompt: \"Classify the sentiment of this review: ...\"\n",
        "* Few Shot Prompt: \"Classify the sentiment of this review based on these examples: ...\"\n",
        "* Chain Of Thought Prompt: \"Classify the sentiment of this review based on these examples and explanations of the reasoning behind: ...\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c649bc16",
      "metadata": {
        "id": "c649bc16"
      },
      "source": [
        "## Few Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c285419d",
      "metadata": {
        "id": "c285419d"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import FewShotChatMessagePromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ae0df0-44fe-4fd2-89a5-bac45a84e612",
      "metadata": {
        "id": "f3ae0df0-44fe-4fd2-89a5-bac45a84e612"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\"input\": \"hi!\", \"output\": \"Â¡hola!\"},\n",
        "    {\"input\": \"bye!\", \"output\": \"Â¡adiÃ³s!\"},\n",
        "]\n",
        "\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an English-Spanish translator.\"),\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“Œ Step 9: Creating a Few-Shot Prompt for Chat Messages**\n",
        "\n",
        "## **ðŸŽ¯ Objective**\n",
        "- **Demonstrate** how to incorporate **few-shot examples** into a chat-based prompt.\n",
        "- **Illustrate** how these examples guide the model by providing reference interactions (in this case, for English-to-Spanish translations).\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ”¹ Code Snippet**\n",
        "\n",
        "    from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "    examples = [\n",
        "        {\"input\": \"hi!\", \"output\": \"Â¡hola!\"},\n",
        "        {\"input\": \"bye!\", \"output\": \"Â¡adiÃ³s!\"},\n",
        "    ]\n",
        "\n",
        "    example_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"human\", \"{input}\"),\n",
        "            (\"ai\", \"{output}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "        example_prompt=example_prompt,\n",
        "        examples=examples,\n",
        "    )\n",
        "\n",
        "    final_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an English-Spanish translator.\"),\n",
        "            few_shot_prompt,\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "---\n",
        "\n",
        "## **1ï¸âƒ£ Few-Shot Learning with `FewShotChatMessagePromptTemplate`**\n",
        "- **Few-Shot Learning**: Providing **sample inputs and outputs** helps the model understand the type of response you expect.\n",
        "- **`examples`**: Here, we have two example pairsâ€”`hi!` â†’ `Â¡hola!`, `bye!` â†’ `Â¡adiÃ³s!`.\n",
        "- **Why It Helps**: The LLM can see how to respond in similar contexts (English â†” Spanish translations).\n",
        "\n",
        "---\n",
        "\n",
        "## **2ï¸âƒ£ `example_prompt` Construction**\n",
        "- **Structure**: `(role, template_string)` pairs define the conversationâ€™s roles: â€œhumanâ€ for input, â€œaiâ€ for output.\n",
        "- **Placeholders**: `\"{input}\"` and `\"{output}\"` can be replaced by each exampleâ€™s values.\n",
        "- This forms the **template** that each example in `examples` will follow.\n",
        "\n",
        "---\n",
        "\n",
        "## **3ï¸âƒ£ `FewShotChatMessagePromptTemplate(...)`**\n",
        "- **Combines** `example_prompt` with the **list of examples**.\n",
        "- This method **automatically replicates** the example prompt structure for each entry in `examples`.\n",
        "\n",
        "---\n",
        "\n",
        "## **4ï¸âƒ£ `final_prompt` Setup**\n",
        "- **System Role**: `\"You are an English-Spanish translator.\"` sets the context for the AI.\n",
        "- **`few_shot_prompt`**: Inserts the previously defined examples to show how translation is done.\n",
        "- **(`human`, \"{input}\")**: The final user query placeholder that will be replaced at runtime.\n",
        "\n",
        "---\n",
        "\n",
        "## **ðŸ“Œ Key Takeaways**\n",
        "1. **Few-Shot Prompting**: Gives the model concrete examples to emulate, increasing **accuracy** and **coherence** of responses.  \n",
        "2. **Flexible Structure**: Each example follows a consistent pattern, making it easy to scale up with more examples.  \n",
        "3. **Context Enrichment**: The **system** message clarifies the modelâ€™s role (translator), while **few-shot** examples guide style and format.\n",
        "\n",
        "> *Think of Few-Shot prompts as giving the model practice runsâ€”small examples that illustrate exactly how you want it to perform before the main event!*\n"
      ],
      "metadata": {
        "id": "Z-da3T9y7M3g"
      },
      "id": "Z-da3T9y7M3g"
    },
    {
      "cell_type": "markdown",
      "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444",
      "metadata": {
        "id": "512a50eb-dfe4-4d08-802e-c3dbc40a3444"
      },
      "source": [
        "## How to execute the code from Visual Studio Code\n",
        "* In Visual Studio Code, see the file 001-connect-llms.py\n",
        "* In terminal, make sure you are in the directory of the file and run:\n",
        "    * python 003-prompt-templates.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069",
      "metadata": {
        "id": "9d0d7c2f-57ed-43f5-b6ed-77c54243c069"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}