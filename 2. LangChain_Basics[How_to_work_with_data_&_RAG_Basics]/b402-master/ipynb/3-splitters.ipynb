{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **3-splitters**\n",
        "\n",
        "## **ü§ñ Introduction**\n",
        "- **Splitters** help **divide** large documents into **smaller, more manageable parts**, making it easier for an LLM to process text without hitting token limits or losing context.\n",
        "\n",
        "## **‚öôÔ∏è Setup**\n",
        "1. **Clone or Download** the GitHub repository to your machine.\n",
        "2. In **terminal**:\n",
        "   ```\n",
        "   cd project_name\n",
        "   pyenv local 3.11.4\n",
        "   poetry install\n",
        "   poetry shell\n",
        "   ```\n",
        "3. Launch **Jupyter Lab**:\n",
        "   ```\n",
        "   jupyter lab\n",
        "   ```\n",
        "   - Open the **`002-splitters.ipynb`** notebook.\n",
        "4. **View Code** in your editor of choice (e.g., VS Code):\n",
        "   - Locate and open **`002-splitters.py`**.\n",
        "\n",
        "---\n",
        "\n",
        "## **üîê Create Your `.env` File**\n",
        "- **`.env.example`** is included; rename it to **`.env`**.\n",
        "- Add the following keys:\n",
        "  ```\n",
        "  OPENAI_API_KEY=your_openai_api_key\n",
        "  LANGCHAIN_TRACING_V2=true\n",
        "  LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
        "  LANGCHAIN_API_KEY=your_langchain_api_key\n",
        "  LANGCHAIN_PROJECT=your_project_name\n",
        "  ```\n",
        "- This project is named **`002-splitters`** in **LangSmith**.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìä Track Operations**\n",
        "- **Monitor** usage and costs for this project in **LangSmith**:\n",
        "  ```\n",
        "  smith.langchain.com\n",
        "  ```\n",
        "\n",
        "> **üí° Note**: Splitting large documents into smaller segments often **improves** LLM performance, as each chunk is easier to process and analyze.\n"
      ],
      "metadata": {
        "id": "31swA8E9SdPu"
      },
      "id": "31swA8E9SdPu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Notebook Summary & Objective  \n",
        "This Jupyter Notebook focuses on **text splitting techniques** using LangChain's **splitters**. It provides an in-depth look at how to divide large data assets into smaller, manageable parts, which is essential for **Natural Language Processing (NLP)** and **LLM-based applications**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Notebook Overview  \n",
        "- **Total Cells:** 49  \n",
        "- **Code Cells:** 24  \n",
        "- **Markdown Cells:** 25  \n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Key Sections in the Notebook  \n",
        "\n",
        "### **1Ô∏è‚É£ Introduction to Splitters**  \n",
        "   - Explains the importance of splitting large texts into smaller chunks.\n",
        "\n",
        "### **2Ô∏è‚É£ Setup & Installation**  \n",
        "   - Installs required libraries like `python-dotenv` and `langchain`.  \n",
        "   - Sets up OpenAI API keys using environment variables.\n",
        "\n",
        "### **3Ô∏è‚É£ Using LangChain Splitters**  \n",
        "   - Demonstrates different methods of text splitting.  \n",
        "   - Uses `CharacterTextSplitter` and possibly other splitters like `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "### **4Ô∏è‚É£ Practical Examples**  \n",
        "   - Splits text using different strategies, showing **how chunk size and overlap affect results**.\n",
        "\n",
        "### **5Ô∏è‚É£ Comparison of Different Splitters**  \n",
        "   - Evaluates **efficiency and performance** of various text-splitting techniques.\n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ Next Steps  \n",
        "Since this notebook is about **text splitters**, please share the **specific code sections** you want me to explain in extreme detail. üöÄ  \n"
      ],
      "metadata": {
        "id": "X3K6SGmt2wGW"
      },
      "id": "X3K6SGmt2wGW"
    },
    {
      "cell_type": "markdown",
      "id": "4f99504a-1b8f-4360-b342-0b81ffa06aff",
      "metadata": {
        "id": "4f99504a-1b8f-4360-b342-0b81ffa06aff"
      },
      "source": [
        "## Connect with the .env file located in the same directory of this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39e5789-5bde-42e1-88dd-92dc8e363c24",
      "metadata": {
        "id": "e39e5789-5bde-42e1-88dd-92dc8e363c24"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c",
      "metadata": {
        "id": "c5514113-ddca-4ae9-9de6-0b9225b18f3c"
      },
      "outputs": [],
      "source": [
        "#pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
      "metadata": {
        "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
      "metadata": {
        "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80"
      },
      "source": [
        "#### Install LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba",
      "metadata": {
        "id": "9fef1e5c-b7e2-4a04-96c5-8f64377b8eba"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
      "metadata": {
        "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
      "metadata": {
        "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941"
      },
      "source": [
        "## Connect with an LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee",
      "metadata": {
        "id": "b21d23f4-61f5-4227-8a75-7eefde6680ee"
      },
      "source": [
        "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
      "metadata": {
        "id": "148df8e0-361d-4ddd-8709-af48fa1648d1"
      },
      "outputs": [],
      "source": [
        "#!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
      "metadata": {
        "id": "a1998155-91de-4cbc-bc88-8d77beefb51b"
      },
      "source": [
        "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Character Splitter in RAG**\n",
        "\n",
        "### **üìö Why We Use RAG (Retrieval-Augmented Generation)**\n",
        "When dealing with large documents, simply passing the entire text to an LLM often **exceeds** the model‚Äôs context window. **RAG** solves this by:\n",
        "1. **Split** the document into **small chunks** (so each chunk can fit into the LLM context).  \n",
        "2. **Transform** these text chunks into **numeric embeddings**.  \n",
        "3. **Store** those embeddings in a **vector database** (vector store).  \n",
        "4. **Retrieve** the most relevant chunks when a user asks a question.  \n",
        "5. **Send** the retrieved embeddings (or text) to the LLM so it can generate a final, **context-rich** response.\n",
        "\n",
        "### **‚úÇÔ∏è Splitters (Document Transformers)**\n",
        "- **Role**: They divide a loaded document into **manageable** segments of text.\n",
        "- **Name**: Sometimes called **‚ÄúDocument Transformers‚Äù** because they transform the raw document into smaller parts.\n",
        "- **Built-In Splitters**: LangChain provides a variety of splitters to handle different data structures. For more details, refer to:\n",
        "  - [Documentation Page](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitter)\n",
        "  - [List of Built-In Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers)\n",
        "\n",
        "### **üí° Character Splitter**\n",
        "- **Default Delimiter**: `\"\\n\\n\"` (two consecutive newline characters).  \n",
        "- **Chunk Measurement**: Based on the **number of characters** in each segment.  \n",
        "- **Purpose**: Break large blocks of text into **smaller pieces**, improving the **efficiency** and **accuracy** of retrieval.\n",
        "\n",
        "#### **How It Works**\n",
        "1. **Scan** the text for `\"\\n\\n\"` delimiters.  \n",
        "2. **Split** the text at these delimiters, producing discrete chunks.  \n",
        "3. **Measure** chunk length in characters to ensure no chunk exceeds your desired size limit.\n",
        "\n",
        "### **üîé Advantages of the Character Splitter**\n",
        "- **Efficient Retrieval**: Smaller chunks make it easier for the system to find the **most relevant** section during the retrieval step of RAG.  \n",
        "- **Enhanced Context Control**: By splitting paragraphs or sections at `\"\\n\\n\"`, each chunk remains a **cohesive** unit of meaning (e.g., a paragraph).  \n",
        "- **Easy Customization**: If your text doesn‚Äôt use double newlines, you can specify a different character or adjust chunk sizes.\n",
        "\n",
        "### **üöÄ Example in RAG**\n",
        "Imagine you have a lengthy research paper with multiple paragraphs separated by blank lines:\n",
        "- **Before**: One huge string that might exceed the LLM‚Äôs token limit.  \n",
        "- **After**: Multiple smaller chunks (paragraph by paragraph) that you can individually embed and store.  \n",
        "- **Query**: When the user asks a question, only the **relevant** paragraph-chunks are retrieved from the vector store and provided to the LLM, leading to a **precise** and **contextual** answer.\n",
        "\n",
        "### **ü§î Key Takeaways**\n",
        "- **Chunking** is crucial for large texts‚Äî**Character Splitter** is the simplest method, relying on newline delimiters.  \n",
        "- This approach **reduces** token usage, making RAG pipelines more **scalable**.  \n",
        "- If your document doesn‚Äôt naturally separate paragraphs with `\"\\n\\n\"`, you can change the delimiter or use a more advanced splitter (like **Recursive Character Splitter**, which respects sentence boundaries or sections).\n",
        "\n",
        "> **üíº Final Note**: Proper splitting is the foundation of a successful RAG strategy‚Äîwithout well-structured chunks, retrieval can become **less accurate** and hamper the LLM‚Äôs ability to provide reliable answers.\n"
      ],
      "metadata": {
        "id": "TNv2Thy1S2XB"
      },
      "id": "TNv2Thy1S2XB"
    },
    {
      "cell_type": "markdown",
      "id": "db520c85-d916-4465-9a53-ee6a7c0b722d",
      "metadata": {
        "id": "db520c85-d916-4465-9a53-ee6a7c0b722d"
      },
      "source": [
        "Here's a simple example to illustrate how the \"Character Splitter\" works in the context of RAG applications using the default delimiter (\"\\n\\n\").\n",
        "\n",
        "#### Original Text:\n",
        "```\n",
        "Hello, welcome to our store!\n",
        "\n",
        "\\n\\nWe offer a variety of products ranging from electronics to clothing.\n",
        "\n",
        "\\n\\nOur store hours are 9 AM to 9 PM every day.\n",
        "\n",
        "\\n\\nFeel free to ask for assistance if you need help finding anything.\n",
        "```\n",
        "\n",
        "#### After Applying Character Splitter:\n",
        "1. **Chunk 1:**\n",
        "   ```\n",
        "   Hello, welcome to our store!\n",
        "   ```\n",
        "\n",
        "2. **Chunk 2:**\n",
        "   ```\n",
        "   We offer a variety of products ranging from electronics to clothing.\n",
        "   ```\n",
        "\n",
        "3. **Chunk 3:**\n",
        "   ```\n",
        "   Our store hours are 9 AM to 9 PM every day.\n",
        "   ```\n",
        "\n",
        "4. **Chunk 4:**\n",
        "   ```\n",
        "   Feel free to ask for assistance if you need help finding anything.\n",
        "   ```\n",
        "\n",
        "In this example, the text is split into four chunks based on the presence of \"\\n\\n\" between sections of text. Each chunk is a manageable size and clearly separated from the others, making it easier for a RAG system to handle and retrieve information from specific parts of the text as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aba8ec6a-9988-493f-9bc2-a1d72eb095b6",
      "metadata": {
        "id": "aba8ec6a-9988-493f-9bc2-a1d72eb095b6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"./data/be-good.txt\")\n",
        "\n",
        "loaded_data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9080df7c-9e81-4d2d-8713-9345e2ac8c54",
      "metadata": {
        "id": "9080df7c-9e81-4d2d-8713-9345e2ac8c54"
      },
      "outputs": [],
      "source": [
        "#loaded_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ecb63be-4c91-4bec-ba71-5d05dc962722",
      "metadata": {
        "id": "7ecb63be-4c91-4bec-ba71-5d05dc962722"
      },
      "outputs": [],
      "source": [
        "#loaded_data[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "063a35f4-da0c-4915-978d-be3d9f70befa",
      "metadata": {
        "id": "063a35f4-da0c-4915-978d-be3d9f70befa"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Code Explanation: Splitting Text into Chunks using `CharacterTextSplitter` from LangChain  \n",
        "\n",
        "## üîπ Code Breakdown  \n",
        "\n",
        "### **1Ô∏è‚É£ Importing the Required Module**  \n",
        "```python\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "```\n",
        "- This imports the `CharacterTextSplitter` class from the `langchain_text_splitters` module.\n",
        "- It is used to **split long text into smaller chunks** for better processing in Language Model (LLM) applications.\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Creating an Instance of `CharacterTextSplitter`**  \n",
        "```python\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "```\n",
        "This **configures the text splitting strategy** with the following parameters:\n",
        "\n",
        "- **`separator=\"\\n\\n\"`**  \n",
        "  - Defines **double newlines (`\\n\\n`)** as the splitting point.\n",
        "  - Useful for **paragraph-based** splitting, ensuring that logical sections are preserved.\n",
        "\n",
        "- **`chunk_size=1000`**  \n",
        "  - Specifies that **each chunk should not exceed 1000 characters**.\n",
        "  - Ensures that **large text blocks** are divided into **manageable parts** for LLM processing.\n",
        "\n",
        "- **`chunk_overlap=200`**  \n",
        "  - Ensures **each chunk overlaps the next by 200 characters**.\n",
        "  - Helps **retain context** across chunks, preventing abrupt cut-offs.\n",
        "\n",
        "- **`length_function=len`**  \n",
        "  - Uses Python‚Äôs built-in `len()` function to **measure chunk length** in **characters**.\n",
        "\n",
        "- **`is_separator_regex=False`**  \n",
        "  - Specifies that the `separator` is **a plain string** and **not a regex pattern**.\n",
        "  - If `True`, advanced **regular expressions** could be used for more complex splitting logic.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Why Use `CharacterTextSplitter`?**\n",
        "- üèÜ **Optimized for LLM Processing**: Splits long texts into **manageable** segments.  \n",
        "- üîÑ **Maintains Context Across Chunks**: Overlapping chunks ensure **smooth transitions**.  \n",
        "- üìñ **Preserves Paragraph Boundaries**: Helps retain **semantic meaning** while chunking.  \n",
        "- ‚ö° **Improves Vector Search & Retrieval**: Essential for **embedding-based** search applications.  \n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Example Usage**\n",
        "```python\n",
        "text = \"This is paragraph one.\\n\\nThis is paragraph two.\\n\\nThis is paragraph three.\"\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "for idx, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {idx + 1}:\")\n",
        "    print(chunk)\n",
        "    print(\"-\" * 50)\n",
        "```\n",
        "\n",
        "### **üîπ Expected Output**\n",
        "```\n",
        "Chunk 1:\n",
        "This is paragraph one.\n",
        "\n",
        "This is paragraph two.\n",
        "--------------------------------------------------\n",
        "Chunk 2:\n",
        "This is paragraph two.\n",
        "\n",
        "This is paragraph three.\n",
        "--------------------------------------------------\n",
        "```\n",
        "- The text is split into **paragraph-based chunks** with an overlap of **200 characters**.  \n",
        "- The second paragraph appears in **both chunks**, ensuring **context retention**.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **When to Use This?**\n",
        "‚úÖ Preparing text for **LLM embeddings** and **vector databases**.  \n",
        "‚úÖ Splitting **long articles, books, or transcripts** into structured sections.  \n",
        "‚úÖ Improving **retrieval accuracy** in search-based applications.  \n",
        "\n",
        "This method ensures **structured and contextual chunking** for better text processing in **LangChain-based AI applications**. üöÄ  \n"
      ],
      "metadata": {
        "id": "cbLr2XnF3F0L"
      },
      "id": "cbLr2XnF3F0L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a1f58df-c7b1-491c-93a1-c065591b28e4",
      "metadata": {
        "id": "1a1f58df-c7b1-491c-93a1-c065591b28e4"
      },
      "outputs": [],
      "source": [
        "texts = text_splitter.create_documents([loaded_data[0].page_content])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Code Explanation: Creating Document Chunks using `CharacterTextSplitter`\n",
        "\n",
        "## üîπ Code Breakdown  \n",
        "\n",
        "### **1Ô∏è‚É£ Executing the Text Splitting Operation**  \n",
        "```python\n",
        "texts = text_splitter.create_documents([loaded_data[0].page_content])\n",
        "```\n",
        "This line **splits the content of a document into multiple chunks** using the `CharacterTextSplitter` instance (`text_splitter`) that was previously configured.\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Understanding Each Component**  \n",
        "\n",
        "#### **‚úÖ `loaded_data[0].page_content`**  \n",
        "- `loaded_data` is expected to be a **list of documents**.  \n",
        "- `loaded_data[0]` refers to **the first document** in the list.  \n",
        "- `.page_content` extracts the **textual content** of that document.\n",
        "\n",
        "#### **‚úÖ `text_splitter.create_documents([...])`**  \n",
        "- The `create_documents()` function **splits the input text into multiple document chunks**.  \n",
        "- It **automatically structures** the output into `Document` objects (LangChain's format), which contain:  \n",
        "  - **Chunked text**  \n",
        "  - **Metadata (if any was present in the original document)**  \n",
        "\n",
        "- The method applies the **splitting strategy** defined earlier:\n",
        "  - **Splitting at `\"\\n\\n\"` (double newlines)**\n",
        "  - **Each chunk has a max size of 1000 characters**\n",
        "  - **Each chunk overlaps the next by 200 characters**\n",
        "  - **Length is measured using `len()`**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Expected Outcome**\n",
        "- `texts` will be a **list of Document objects**, each containing a **text chunk**.\n",
        "- Each chunk will have **overlapping content** to maintain context.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Example Output**\n",
        "If `loaded_data[0].page_content` contains:\n",
        "```\n",
        "Paragraph 1.\n",
        "\n",
        "Paragraph 2.\n",
        "\n",
        "Paragraph 3.\n",
        "\n",
        "Paragraph 4.\n",
        "```\n",
        "Then `texts` might contain:\n",
        "```\n",
        "[ Document(page_content=\"Paragraph 1.\\n\\nParagraph 2.\"),\n",
        "  Document(page_content=\"Paragraph 2.\\n\\nParagraph 3.\"),\n",
        "  Document(page_content=\"Paragraph 3.\\n\\nParagraph 4.\") ]\n",
        "```\n",
        "Each chunk **overlaps by one paragraph** due to the **200-character overlap** setting.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **When to Use This?**\n",
        "‚úÖ **Preparing text chunks** for retrieval-augmented generation (RAG).  \n",
        "‚úÖ **Feeding structured chunks** into embeddings-based search engines.  \n",
        "‚úÖ **Improving LLM document processing** by ensuring context continuity.  \n",
        "\n",
        "This ensures **efficient text chunking** for **LLM-based applications**, preserving context while splitting long texts. üöÄ  \n"
      ],
      "metadata": {
        "id": "glPWl6Mp3PWq"
      },
      "id": "glPWl6Mp3PWq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c2bf32e-3cc8-4b59-bc8d-5617b37a62fb",
      "metadata": {
        "id": "8c2bf32e-3cc8-4b59-bc8d-5617b37a62fb",
        "outputId": "4df84512-346d-4962-cdb8-faeb0a2c87f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c9e310-597b-46aa-accf-69eeda77f7f5",
      "metadata": {
        "id": "97c9e310-597b-46aa-accf-69eeda77f7f5",
        "outputId": "4471dc01-1a2d-4874-d19d-4c65c56edd66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Be good')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "297dd163-7d2c-476b-b332-c2bf232461d5",
      "metadata": {
        "id": "297dd163-7d2c-476b-b332-c2bf232461d5"
      },
      "outputs": [],
      "source": [
        "# texts[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c9b427-4002-4995-9db4-63d5afa9403c",
      "metadata": {
        "id": "b8c9b427-4002-4995-9db4-63d5afa9403c"
      },
      "source": [
        "#### Splitting with metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "becc89c4-ec8c-468a-af28-711184d9c75c",
      "metadata": {
        "id": "becc89c4-ec8c-468a-af28-711184d9c75c"
      },
      "outputs": [],
      "source": [
        "metadatas = [{\"chunk\": 0}, {\"chunk\": 1}]\n",
        "\n",
        "documents = text_splitter.create_documents(\n",
        "    [loaded_data[0].page_content, loaded_data[0].page_content],\n",
        "    metadatas=metadatas\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Code Explanation: Creating Document Chunks with Metadata using `CharacterTextSplitter`\n",
        "\n",
        "## üîπ Code Breakdown  \n",
        "\n",
        "### **1Ô∏è‚É£ Defining Metadata for Each Chunk**  \n",
        "```python\n",
        "metadatas = [{\"chunk\": 0}, {\"chunk\": 1}]\n",
        "```\n",
        "- A list of **metadata dictionaries** is created.  \n",
        "- Each dictionary represents metadata for **one chunk**.\n",
        "- Example:\n",
        "  - `{\"chunk\": 0}` ‚Üí Metadata for the **first chunk**.\n",
        "  - `{\"chunk\": 1}` ‚Üí Metadata for the **second chunk**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Creating Document Chunks with Metadata**  \n",
        "```python\n",
        "documents = text_splitter.create_documents(\n",
        "    [loaded_data[0].page_content, loaded_data[0].page_content],\n",
        "    metadatas=metadatas\n",
        ")\n",
        "```\n",
        "- `create_documents()` takes **two inputs**:\n",
        "  1. A list of **texts** (here, the **same document is duplicated** for demonstration).  \n",
        "  2. A list of **metadata dictionaries** (`metadatas`), which attaches metadata to each chunk.  \n",
        "\n",
        "- The text is **split into chunks** using the **configured splitting strategy**:\n",
        "  - **Splitting at `\"\\n\\n\"` (double newlines)**\n",
        "  - **Each chunk has a max size of 1000 characters**\n",
        "  - **Each chunk overlaps by 200 characters**\n",
        "  - **Length is measured using `len()`**\n",
        "  - **Metadata is assigned per chunk**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Understanding the Parameters in Context**\n",
        "### **‚úÖ CharacterTextSplitter Configuration Recap**\n",
        "| Parameter           | Description |\n",
        "|---------------------|-------------|\n",
        "| `separator=\"\\n\\n\"` | Splits text at **double newlines**, ensuring logical chunking. |\n",
        "| `chunk_size=1000` | Each chunk is **max 1000 characters long**. |\n",
        "| `chunk_overlap=200` | Each chunk **overlaps by 200 characters**, retaining context. |\n",
        "| `length_function=len` | Uses `len()` to measure chunk size in **characters**. |\n",
        "| `is_separator_regex=False` | Treats `separator` as **plain text**, not a regex pattern. |\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Example Output**\n",
        "If `loaded_data[0].page_content` contains:\n",
        "```\n",
        "Paragraph 1.\n",
        "\n",
        "Paragraph 2.\n",
        "\n",
        "Paragraph 3.\n",
        "\n",
        "Paragraph 4.\n",
        "```\n",
        "Then `documents` might contain:\n",
        "```\n",
        "[ Document(page_content=\"Paragraph 1.\\n\\nParagraph 2.\", metadata={\"chunk\": 0}),\n",
        "  Document(page_content=\"Paragraph 2.\\n\\nParagraph 3.\", metadata={\"chunk\": 1}) ]\n",
        "```\n",
        "- The text is split into **document chunks**.\n",
        "- Each chunk is **overlapping** the next by 200 characters.\n",
        "- Metadata **assigns identifiers** to the chunks.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Why Attach Metadata?**\n",
        "‚úÖ **Maintains Sequential Information** ‚Üí Keeps track of which chunk comes first.  \n",
        "‚úÖ **Improves Document Retrieval** ‚Üí Helps identify chunks in **vector search**.  \n",
        "‚úÖ **Enhances Processing** ‚Üí Metadata can store **timestamps, categories, or sources**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ **When to Use This?**\n",
        "- **Retrieval-Augmented Generation (RAG)** ‚Üí When sending **chunks** to an LLM.\n",
        "- **Document Indexing for Search** ‚Üí When embedding **text into a vector database**.\n",
        "- **Context-Aware AI Applications** ‚Üí When **metadata needs to be preserved**.  \n",
        "\n",
        "This ensures **structured text processing** for AI-powered applications! üöÄ  \n"
      ],
      "metadata": {
        "id": "sfNiFax64nBE"
      },
      "id": "sfNiFax64nBE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f32ee1a-dcbc-4906-9e26-ff1c00f43e30",
      "metadata": {
        "id": "2f32ee1a-dcbc-4906-9e26-ff1c00f43e30",
        "outputId": "266ed0c7-bdf3-450d-87bb-cfcdb26d1a49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'chunk': 0}, page_content='Be good')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Code Explanation: Accessing a Specific Document Chunk  \n",
        "\n",
        "## üîπ Code Breakdown  \n",
        "\n",
        "### **1Ô∏è‚É£ Accessing the First Chunked Document**  \n",
        "```python\n",
        "documents[0]\n",
        "```\n",
        "- This retrieves **the first document chunk** from the `documents` list.  \n",
        "- `documents` was created using `text_splitter.create_documents()`.  \n",
        "- Since the **splitting strategy** was applied, `documents` now contains multiple **chunked documents**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Expected Output**\n",
        "```python\n",
        "Document(metadata={'chunk': 0}, page_content='Be good')\n",
        "```\n",
        "- **`Document(...)`** ‚Üí Represents a **LangChain `Document` object**.  \n",
        "- **`metadata={'chunk': 0}`** ‚Üí This **metadata dictionary** assigns an **identifier** to the chunk.  \n",
        "  - Here, `chunk: 0` indicates that this is **the first chunk**.  \n",
        "- **`page_content='Be good'`** ‚Üí The **actual content** of this document chunk.  \n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Understanding What Happened**\n",
        "1Ô∏è‚É£ **The text was split** into chunks based on the pre-configured `CharacterTextSplitter`.  \n",
        "2Ô∏è‚É£ **Metadata was attached** to track chunk numbers.  \n",
        "3Ô∏è‚É£ **`documents[0]` returned** the first chunk, which contains:  \n",
        "   - **A metadata dictionary** (`{\"chunk\": 0}`)  \n",
        "   - **A text snippet** (`\"Be good\"`)  \n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Why is This Useful?**\n",
        "‚úÖ **Chunk Tracking** ‚Üí Each document retains **metadata**, helping with indexing.  \n",
        "‚úÖ **Retrieval-Augmented Generation (RAG)** ‚Üí LLMs can reference structured text chunks.  \n",
        "‚úÖ **Efficient Document Processing** ‚Üí Useful for **semantic search & embeddings**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ **Next Steps**\n",
        "- To access the **second document chunk**:\n",
        "  ```python\n",
        "  documents[1]\n",
        "  ```\n",
        "- To view **all document chunks**:\n",
        "  ```python\n",
        "  for doc in documents:\n",
        "      print(doc)\n",
        "  ```\n",
        "- To extract **text content only**:\n",
        "  ```python\n",
        "  print(documents[0].page_content)\n",
        "  ```\n",
        "\n",
        "This method ensures **structured document processing** for **AI applications**! üöÄ  \n"
      ],
      "metadata": {
        "id": "JIp6F_4Y70rX"
      },
      "id": "JIp6F_4Y70rX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a892bfb2-d1ab-4e9d-a4b2-25a4a6319577",
      "metadata": {
        "id": "a892bfb2-d1ab-4e9d-a4b2-25a4a6319577",
        "outputId": "adb0f7d9-20bd-4ed2-a6c8-3ba40659047d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='Be good' metadata={'chunk': 0}\n"
          ]
        }
      ],
      "source": [
        "print(documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Code Explanation: Printing a Document Chunk  \n",
        "\n",
        "## üîπ Code Breakdown  \n",
        "\n",
        "### **1Ô∏è‚É£ Printing the First Document Chunk**  \n",
        "```python\n",
        "print(documents[0])\n",
        "```\n",
        "- This prints the **first document chunk** stored in the `documents` list.\n",
        "- `documents` was created using `text_splitter.create_documents()`, which **split the text** and **attached metadata**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Expected Output**\n",
        "```python\n",
        "page_content='Be good' metadata={'chunk': 0}\n",
        "```\n",
        "- **`page_content='Be good'`** ‚Üí This is the **actual text** stored in this chunk.  \n",
        "- **`metadata={'chunk': 0}`** ‚Üí This **tracks the chunk index** in the splitting process.  \n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Breaking Down What Happened**\n",
        "### **‚úÖ Step 1: Text Splitting**\n",
        "- The original text was **split into chunks** using `CharacterTextSplitter`.\n",
        "- Since `chunk_size=1000` and `separator=\"\\n\\n\"`, the first chunk **only contains `\"Be good\"`**.\n",
        "\n",
        "### **‚úÖ Step 2: Metadata Assignment**\n",
        "- `{\"chunk\": 0}` is attached to **track** the chunk number.\n",
        "- If there were more chunks, they would have `chunk: 1`, `chunk: 2`, etc.\n",
        "\n",
        "### **‚úÖ Step 3: Printing the First Chunk**\n",
        "- `documents[0]` is an instance of LangChain‚Äôs `Document` class.\n",
        "- When printed, it displays the **text (`page_content`)** and its **metadata**.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Why is This Useful?**\n",
        "‚úÖ **Helps Track Chunk Order** ‚Üí Useful for document indexing.  \n",
        "‚úÖ **Preserves Context** ‚Üí Metadata allows **better retrieval & reference**.  \n",
        "‚úÖ **Essential for Vector Search** ‚Üí Metadata is **critical in embeddings-based applications**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ **Next Steps**\n",
        "- **Print all chunks:**\n",
        "  ```python\n",
        "  for doc in documents:\n",
        "      print(doc)\n",
        "  ```\n",
        "- **Extract only text:**\n",
        "  ```python\n",
        "  print(documents[0].page_content)\n",
        "  ```\n",
        "- **Extract only metadata:**\n",
        "  ```python\n",
        "  print(documents[0].metadata)\n",
        "  ```\n",
        "\n",
        "This ensures **structured text processing** for AI-powered applications! üöÄ  \n"
      ],
      "metadata": {
        "id": "5JC_zcQw7_ln"
      },
      "id": "5JC_zcQw7_ln"
    },
    {
      "cell_type": "markdown",
      "id": "ea671270",
      "metadata": {
        "id": "ea671270"
      },
      "source": [
        "## Recursive Character Splitter\n",
        "* This text splitter is the recommended one for generic text.\n",
        "* It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
        "* This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c24c6b-f163-47da-9282-02d3f14b947f",
      "metadata": {
        "id": "95c24c6b-f163-47da-9282-02d3f14b947f"
      },
      "source": [
        "# üìå Simple Explanation: Recursive Character Splitter  \n",
        "\n",
        "## üîπ What is the Recursive Character Splitter?  \n",
        "- The **\"Recursive Character Splitter\"** is a method used to **divide text into smaller, more manageable chunks**.  \n",
        "- It is designed to **preserve the semantic integrity** of the text while ensuring optimal chunking.  \n",
        "- The process follows a **hierarchical splitting strategy**, meaning it first tries to split by **larger units** (paragraphs), then by **smaller units** (sentences, words) if necessary.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ How Does It Work?  \n",
        "- It attempts to split text using **a prioritized list of characters** in a specified order.\n",
        "- The **default splitting sequence** is:  \n",
        "  1Ô∏è‚É£ `\"\\n\\n\"` ‚Üí First, split at **double newlines** to separate **paragraphs**.  \n",
        "  2Ô∏è‚É£ `\"\\n\"` ‚Üí Then, split at **single newlines** to isolate **sentences**.  \n",
        "  3Ô∏è‚É£ `\" \"` ‚Üí If still too large, split at **spaces** to break into **phrases**.  \n",
        "  4Ô∏è‚É£ `\"\"` ‚Üí As a last resort, split at **each character individually**.  \n",
        "\n",
        "- The algorithm ensures that **each chunk retains meaningful, complete information**.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Example Usage**  \n",
        "\n",
        "### **‚úÖ Original Text:**  \n",
        "```\n",
        "Hello, welcome to our store!\n",
        "\n",
        "\\n\\nWe offer a variety of products. Our range includes electronics, clothing, and home appliances.\n",
        "\\nOur staff is available to help you during store hours: 9 AM to 9 PM every day.\n",
        "```\n",
        "\n",
        "### **‚úÖ Applying Recursive Character Splitter:**  \n",
        "\n",
        "#### **üîπ First Attempt (Splitting at `\"\\n\\n\"` - Paragraphs)**\n",
        "1Ô∏è‚É£ **Chunk 1:**  \n",
        "```\n",
        "Hello, welcome to our store!\n",
        "```\n",
        "2Ô∏è‚É£ **Chunk 2:**  \n",
        "```\n",
        "We offer a variety of products. Our range includes electronics, clothing, and home appliances.\n",
        "\\nOur staff is available to help you during store hours: 9 AM to 9 PM every day.\n",
        "```\n",
        "üìå *Paragraph-based splitting works well, but Chunk 2 is still too large.*\n",
        "\n",
        "---\n",
        "\n",
        "#### **üîπ Second Attempt (Splitting at `\"\\n\"` - Sentences)**\n",
        "1Ô∏è‚É£ **Chunk 1:**  \n",
        "```\n",
        "Hello, welcome to our store!\n",
        "```\n",
        "2Ô∏è‚É£ **New Chunk 2:**  \n",
        "```\n",
        "We offer a variety of products. Our range includes electronics, clothing, and home appliances.\n",
        "```\n",
        "3Ô∏è‚É£ **Chunk 3:**  \n",
        "```\n",
        "Our staff is available to help you during store hours: 9 AM to 9 PM every day.\n",
        "```\n",
        "üìå *Now, each chunk is more structured and contains complete information.*\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Why is This Effective?**  \n",
        "‚úÖ **Preserves Context** ‚Üí Ensures each chunk contains **meaningful information**.  \n",
        "‚úÖ **Hierarchical Splitting** ‚Üí Prioritizes **larger units first** before breaking into smaller ones.  \n",
        "‚úÖ **Optimized for Language Models** ‚Üí Helps **LLMs retain context** across chunks.  \n",
        "‚úÖ **Ideal for Document Processing** ‚Üí Works well for **parsing large texts**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ **When to Use This?**  \n",
        "- **LLM-based text processing** (Retrieval-Augmented Generation - RAG).  \n",
        "- **Chunking large articles, books, or documents** for AI applications.  \n",
        "- **Preparing structured text** for **embedding-based search**.  \n",
        "\n",
        "The **Recursive Character Splitter** ensures that chunks are **intelligently divided**, maintaining logical meaning while efficiently managing text size. üöÄ  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9adc17",
      "metadata": {
        "id": "0c9adc17"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f4f0f2f",
      "metadata": {
        "id": "6f4f0f2f"
      },
      "outputs": [],
      "source": [
        "# Creating an instance of RecursiveCharacterTextSplitter\n",
        "recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=26,    # Each chunk will have a maximum of 26 characters\n",
        "    chunk_overlap=4   # Each chunk will overlap the next one by 4 characters\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ What Does This Code Do?  \n",
        "- It initializes **RecursiveCharacterTextSplitter**, a LangChain utility used to **split text into structured chunks**.\n",
        "- This method follows a **hierarchical splitting strategy**, ensuring **semantic integrity** of the text.  \n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Understanding the Parameters  \n",
        "\n",
        "| Parameter | Description |\n",
        "|-----------|-------------|\n",
        "| `chunk_size=26` | Each chunk will contain a maximum of **26 characters**. |\n",
        "| `chunk_overlap=4` | The last **4 characters of a chunk** will appear at the beginning of the next chunk to maintain **context continuity**. |\n",
        "\n",
        "- **Why Overlap?**  \n",
        "  - **Prevents abrupt cut-offs** at sentence boundaries.  \n",
        "  - **Helps LLMs retain context** across different chunks.  \n",
        "  - **Improves retrieval effectiveness** in vector search applications.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Example Usage & Expected Output**  \n",
        "\n",
        "### **‚úÖ Sample Text:**  \n",
        "```\n",
        "\"Welcome to AI development. This is a new era of innovation.\"\n",
        "```\n",
        "\n",
        "### **‚úÖ Applying RecursiveCharacterTextSplitter**\n",
        "```python\n",
        "text = \"Welcome to AI development. This is a new era of innovation.\"\n",
        "chunks = recursive_splitter.split_text(text)\n",
        "\n",
        "for idx, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {idx + 1}: {chunk}\")\n",
        "```\n",
        "\n",
        "### **üîπ Expected Output:**\n",
        "```\n",
        "Chunk 1: Welcome to AI developme\n",
        "Chunk 2: lopment. This is a new\n",
        "Chunk 3: new era of innovation.\n",
        "```\n",
        "üìå **Explanation:**  \n",
        "- The first chunk **fits within 26 characters**: `\"Welcome to AI developme\"`.  \n",
        "- The second chunk **overlaps 4 characters from the previous chunk** (`\"ment\" ‚Üí \"lopment\"`) to **preserve context**.  \n",
        "- This continues until all text is **processed into structured chunks**.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Why Use RecursiveCharacterTextSplitter?**  \n",
        "‚úÖ **Preserves Meaning** ‚Üí Uses **smart splitting** to avoid breaking words.  \n",
        "‚úÖ **Optimized for LLMs** ‚Üí Ensures **coherent text chunking** for AI applications.  \n",
        "‚úÖ **Better Context Retention** ‚Üí Overlapping **prevents abrupt cut-offs**.  \n",
        "‚úÖ **Great for Vector Databases** ‚Üí Works well for **embedding-based retrieval**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ **When to Use This?**  \n",
        "- **Chunking large documents for LLM-based retrieval** (RAG).  \n",
        "- **Preparing structured embeddings** for a **vector database**.  \n",
        "- **Splitting long texts** while preserving **semantic structure**.  \n",
        "\n",
        "The **RecursiveCharacterTextSplitter** intelligently **breaks text into logical chunks**, ensuring **smooth processing for AI-powered applications**. üöÄ  "
      ],
      "metadata": {
        "id": "E9H_FXVh803G"
      },
      "id": "E9H_FXVh803G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d904ab",
      "metadata": {
        "id": "42d904ab"
      },
      "outputs": [],
      "source": [
        "text1 = 'abcdefghijklmnopqrstuvwxyzabcdefg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7396e743",
      "metadata": {
        "id": "7396e743"
      },
      "outputs": [],
      "source": [
        "text2 = \"\"\"\n",
        "Data that Speak\n",
        "LLM Applications are revolutionizing industries such as\n",
        "banking, healthcare, insurance, education, legal, tourism,\n",
        "construction, logistics, marketing, sales, customer service,\n",
        "and even public administration.\n",
        "\n",
        "The aim of our programs is for students to learn how to\n",
        "create LLM Applications in the context of a business,\n",
        "which presents a set of challenges that are important\n",
        "to consider in advance.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2f91ed",
      "metadata": {
        "id": "2a2f91ed",
        "outputId": "6b17a5c8-3650-45a0-d6f4-1efc7ceb59fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recursive_splitter.split_text(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a4b4a21",
      "metadata": {
        "id": "7a4b4a21",
        "outputId": "f768ad86-29d2-4e43-c0fd-24c52c4c6b43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Data that Speak',\n",
              " 'LLM Applications are',\n",
              " 'are revolutionizing',\n",
              " 'industries such as',\n",
              " 'banking, healthcare,',\n",
              " 'insurance, education,',\n",
              " 'legal, tourism,',\n",
              " 'construction, logistics,',\n",
              " 'marketing, sales,',\n",
              " 'customer service,',\n",
              " 'and even public',\n",
              " 'administration.',\n",
              " 'The aim of our programs',\n",
              " 'is for students to learn',\n",
              " 'how to',\n",
              " 'create LLM Applications',\n",
              " 'in the context of a',\n",
              " 'a business,',\n",
              " 'which presents a set of',\n",
              " 'of challenges that are',\n",
              " 'are important',\n",
              " 'to consider in advance.']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recursive_splitter.split_text(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7756d7a2",
      "metadata": {
        "id": "7756d7a2"
      },
      "outputs": [],
      "source": [
        "# Creating an instance of RecursiveCharacterTextSplitter with custom separators\n",
        "second_recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=150,   # Each chunk will have a maximum of 150 characters\n",
        "    chunk_overlap=0,  # No overlapping between chunks\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
        "    # Defines the order in which the text is split:\n",
        "    # 1. Double newline (\"\\n\\n\") - First, try splitting by paragraphs.\n",
        "    # 2. Single newline (\"\\n\") - If needed, split by sentence lines.\n",
        "    # 3. Period with space \"(?<=\\. )\" - Then, split at sentence boundaries.\n",
        "    # 4. Space (\" \") - If still too long, split by words.\n",
        "    # 5. Empty string (\"\") - As a last resort, split at individual characters.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e6acfa6-5c64-417f-a2e1-c1f9939f1305",
      "metadata": {
        "id": "5e6acfa6-5c64-417f-a2e1-c1f9939f1305"
      },
      "source": [
        "# üìå Code Explanation: Recursive Character Text Splitting with Custom Separators  \n",
        "\n",
        "## üîπ What Does This Code Do?  \n",
        "- This initializes **RecursiveCharacterTextSplitter** with **custom separators** to **split text intelligently**.\n",
        "- Instead of just splitting by **character count**, it follows **a prioritized list of breaking points**.\n",
        "- This helps **maintain logical segmentation**, making text chunks **more meaningful**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Understanding the Parameters  \n",
        "\n",
        "| Parameter | Description |\n",
        "|-----------|-------------|\n",
        "| `chunk_size=150` | Each chunk will contain a maximum of **150 characters**. |\n",
        "| `chunk_overlap=0` | **No overlap** between chunks, meaning each chunk is independent. |\n",
        "| `separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]` | **Defines the order in which text should be split.** |\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Understanding the Separators**  \n",
        "\n",
        "The `separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]` define a **hierarchical splitting strategy**, progressively reducing chunk sizes while maintaining the text‚Äôs logical structure.\n",
        "\n",
        "### **‚úÖ Step-by-step Explanation of the Separators**\n",
        "1Ô∏è‚É£ **\"\\n\\n\" (Double Newline) ‚Äì Paragraph Splitting**  \n",
        "   - This separator **targets paragraph boundaries**.  \n",
        "   - **Use case:** Splitting long-form documents (e.g., books, articles) into **separate sections**.  \n",
        "\n",
        "2Ô∏è‚É£ **\"\\n\" (Single Newline) ‚Äì Line Splitting**  \n",
        "   - Targets **single line breaks**, useful when **paragraphs contain multiple short lines**.  \n",
        "   - **Use case:** Lists, poetry, or structured data where **line breaks matter**.  \n",
        "\n",
        "3Ô∏è‚É£ **\"(?<=\\. )\" (Regex-Based Sentence Splitting)**  \n",
        "   - This is a **regular expression lookbehind assertion**, which **splits only after a period (`.`) followed by a space (` `)**.  \n",
        "   - **Use case:** Ensuring sentences stay intact when **splitting prose or structured writing**.  \n",
        "\n",
        "4Ô∏è‚É£ **\" \" (Space) ‚Äì Word Splitting**  \n",
        "   - If text is **still too long**, it splits at spaces **between words**.  \n",
        "   - **Use case:** Extracting **phrases or individual words** when necessary.  \n",
        "\n",
        "5Ô∏è‚É£ **\"\" (Empty String) ‚Äì Character-Level Splitting**  \n",
        "   - As a last resort, it breaks text **into individual characters**.  \n",
        "   - **Use case:** Ensuring **all text chunks fit within `chunk_size=150`** characters.  \n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ **Example Usage & Expected Output**  \n",
        "\n",
        "### **‚úÖ Sample Text:**  \n",
        "```\n",
        "Hello, welcome to our store!\n",
        "\n",
        "We offer a variety of products.\n",
        "Our range includes electronics, clothing, and home appliances.\n",
        "Our staff is available to help you during store hours: 9 AM to 9 PM every day.\n",
        "```\n",
        "\n",
        "### **‚úÖ Applying `RecursiveCharacterTextSplitter`**\n",
        "```python\n",
        "text = \"\"\"Hello, welcome to our store!\\n\\nWe offer a variety of products.\n",
        "Our range includes electronics, clothing, and home appliances.\n",
        "Our staff is available to help you during store hours: 9 AM to 9 PM every day.\"\"\"\n",
        "\n",
        "chunks = second_recursive_splitter.split_text(text)\n",
        "\n",
        "for idx, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {idx + 1}: {chunk}\")\n",
        "```\n",
        "\n",
        "### **üîπ Expected Output:**\n",
        "```\n",
        "Chunk 1: Hello, welcome to our store!\n",
        "Chunk 2: We offer a variety of products.\n",
        "Chunk 3: Our range includes electronics, clothing, and home appliances.\n",
        "Chunk 4: Our staff is available to help you during store hours: 9 AM to 9 PM every day.\n",
        "```\n",
        "üìå **Explanation:**  \n",
        "- **Chunk 1:** The first paragraph is split at `\"\\n\\n\"`.  \n",
        "- **Chunk 2 & 3:** Remaining text is split by sentence using `\"(?<=\\. )\"`.  \n",
        "- **Chunk 4:** Last sentence is retained as a full chunk.  \n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Why Use Custom Separators in RecursiveCharacterTextSplitter?**  \n",
        "‚úÖ **Maintains Logical Segmentation** ‚Üí Ensures that chunks retain their **semantic structure**.  \n",
        "‚úÖ **Preserves Sentence Integrity** ‚Üí Uses **regex-based** splitting to **keep sentences intact**.  \n",
        "‚úÖ **No Unnecessary Fragmentation** ‚Üí **Paragraphs and sentences remain readable**.  \n",
        "‚úÖ **Perfect for LLM Processing** ‚Üí Helps **LLMs retain context** better than arbitrary chunking.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìÇ **When to Use This?**  \n",
        "- **Processing long documents while keeping structure intact**.  \n",
        "- **Preparing well-segmented text for embedding-based retrieval**.  \n",
        "- **Ensuring AI models process full, meaningful sentences** rather than cut-off phrases.  \n",
        "\n",
        "The **RecursiveCharacterTextSplitter** intelligently **breaks text into logical chunks**, ensuring **smooth processing for AI-powered applications**! üöÄ  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45e8a845",
      "metadata": {
        "id": "45e8a845",
        "outputId": "268dd9e4-d7e8-42b9-c9a6-3b1efd090e99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Data that Speak\\nLLM Applications are revolutionizing industries such as \\nbanking, healthcare, insurance, education, legal, tourism,',\n",
              " 'construction, logistics, marketing, sales, customer service, \\nand even public administration.',\n",
              " 'The aim of our programs is for students to learn how to \\ncreate LLM Applications in the context of a business,',\n",
              " 'which presents a set of challenges that are important \\nto consider in advance.']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "second_recursive_splitter.split_text(text2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc24f4f-d23c-4202-992d-91b0623136ae",
      "metadata": {
        "id": "dbc24f4f-d23c-4202-992d-91b0623136ae"
      },
      "source": [
        "## How to execute the code from Visual Studio Code\n",
        "* In Visual Studio Code, see the file 002-splitters.py\n",
        "* In terminal, make sure you are in the directory of the file and run:\n",
        "    * python 002-splitters.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1af248e-6069-44b3-a2cd-a20aa3259874",
      "metadata": {
        "id": "b1af248e-6069-44b3-a2cd-a20aa3259874"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}